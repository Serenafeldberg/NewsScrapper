{
  "category": "AI/Tech",
  "scraped_at": "2025-10-01T13:38:26.742937",
  "total_articles": 29,
  "articles": [
    {
      "title": "AI Chip Maker Raises $1.1B, Valued at $8.1B",
      "description": "Nvidia rival Cerebras announced a Series G funding round almost a year after it filed for an IPO.",
      "url": "https://aibusiness.com/data-centers/ai-chip-maker-raises-1-1b-valued-at-8-1b",
      "source": "AI Business",
      "published_date": "2025-10-01T12:33:29",
      "author": "Graham Hope",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "OpenAI Intros Sora 2 and a Social Media App",
      "description": "The updated multi-modal model aims to improve realism by addressing problems such as the distortion of reality. The app has a customizable feed for discovering and remixing videos.",
      "url": "https://aibusiness.com/generative-ai/openai-intros-sora-2-social-media-app",
      "source": "AI Business",
      "published_date": "2025-09-30T21:44:23",
      "author": "Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "CoreWeave forges $14.2B Contract With Meta for AI Compute",
      "description": "The contract is part of a wave of big AI infrastructure deals as the tech industry looks to ensure compute power for energy-intensive AI workloads into the 2030s.",
      "url": "https://aibusiness.com/cloud-computing/coreweave-forges-contract-meta-ai-compute",
      "source": "AI Business",
      "published_date": "2025-09-30T20:14:08",
      "author": "Shaun Sutner",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Nvidia Pushes Humanoids, Physical AI With New Tools",
      "description": "The upgrades are pitched as providing the \"brains, body and training ground\" for humanoid robots.",
      "url": "https://aibusiness.com/robotics/nvidia-pushes-humanoids-physical-ai-new-tools",
      "source": "AI Business",
      "published_date": "2025-09-30T16:48:02",
      "author": "Scarlett Evans",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "ServiceNow Unveils AI Experience With Agentic Features",
      "description": "The AI Experience offers instant access to a range of AI agents for voice, images and data.",
      "url": "https://aibusiness.com/agentic-ai/servicenow-unveils-ai-experience-agentic-features",
      "source": "AI Business",
      "published_date": "2025-09-30T15:06:00",
      "author": "Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Anthropic Launches Claude Sonnet 4.5",
      "description": "The model was released under the vendor's AI Safety Level 3 protection, which helps prevent dangerous inputs and outputs.",
      "url": "https://aibusiness.com/foundation-models/anthropic-launches-claude-sonnet-4-5",
      "source": "AI Business",
      "published_date": "2025-09-29T21:13:12",
      "author": "Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Industrial Robot Demand Doubled in 10 Years",
      "description": "China is dominating robotic installations, while European countries are beginning to lag behind.",
      "url": "https://aibusiness.com/robotics/industrial-robot-demand-doubled-in-10-years",
      "source": "AI Business",
      "published_date": "2025-09-29T17:40:41",
      "author": "Scarlett Evans",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "DeepMind's Gemini Robotics 1.5 Gives Robots Reasoning Powers",
      "description": "Google's AI research unit said the new models mark a step closer to artificial general intelligence.",
      "url": "https://aibusiness.com/robotics/deepmind-gemini-robotics-reasoning-powers",
      "source": "AI Business",
      "published_date": "2025-09-26T15:52:48",
      "author": "Scarlett Evans",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "IBM and AMD Collaborate with Zyphra on Next Generation AI Infrastructure",
      "description": "Open-source superintelligence company leverages new integrated capabilities for AMD training clusters on IBM Cloud",
      "url": "https://newsroom.ibm.com/2025-10-01-ibm-and-amd-collaborate-with-zyphra-on-next-generation-ai-infrastructure",
      "source": "IBM Think",
      "published_date": "2025-10-01T11:04:00",
      "author": null,
      "content": "ARMONK, N.Y. and AUSTIN, Texas , Oct. 1, 2025 / PRNewswire / -- IBM (NYSE: IBM ) and AMD (NASDAQ: AMD) today announced a collaboration to deliver advanced AI infrastructure to Zyphra, an open-source AI research and product company based in San Francisco, California . Under a multi-year agreement between IBM and Zyphra, IBM is positioned to deliver a large cluster of AMD Instinct™ MI300X GPUs on IBM Cloud for Zyphra to use for training frontier multimodal foundation models. This collaboration is expected to deliver among the largest advanced generative AI training capabilities to date powered by an AMD stack running on IBM Cloud. Zyphra recently closed a Series A financing round at a $1B valuation to build a leading open-source/open-science superintelligence lab focused on advancing fundamental innovations in novel neural network architectures, long-term memory, and continual learning. Zyphra partnered with IBM and AMD for their cutting-edge product roadmaps and ability to deliver GPU accelerators at the rapid pace required to drive Zyphra's innovation forward. This agreement is the first large-scale, dedicated training cluster on IBM Cloud leveraging AMD Instinct MI300X GPUs and AMD Pensando™ Pollara 400 AI NICs and AMD Pensando Ortano DPUs. The initial deployment was made available to Zyphra in early September with planned expansion in 2026. Photo credit: AMD Zyphra will use the advanced training cluster to develop multimodal foundation models across language, vision and audio modalities to power Maia, a general purpose superagent designed to deliver productivity benefits for knowledge workers across enterprise. IBM and AMD are uniquely positioned to continue scaling computational resources as Zyphra's AI model training needs expand. \"This collaboration marks the first time AMD's full-stack training platform—spanning compute through networking—has been successfully integrated and scaled on IBM Cloud, and Zyphra is honored to lead the way in developing frontier models with AMD silicon on IBM Cloud,\" said Krithik Puthalath, CEO and Chairman of Zyphra . \"We're excited to partner with IBM and AMD to power the next era of open-source, enterprise superintelligence.\" IBM and AMD Unlocking a New Era in AI Training Infrastructure Last year, IBM and AMD announced a collaboration to deploy AMD Instinct MI300X accelerators as a service on IBM Cloud. Known for its security, reliability and scalability, IBM Cloud's robust infrastructure complements the capabilities of the AMD Instinct MI300X. This offering was designed to enhance performance and power efficiency for Gen AI models and high-performance computing (HPC) applications. \"Scaling AI workloads faster and more efficiently is a key differentiator in achieving ROI for established enterprises and emerging companies alike,\" said Alan Peacock , GM of IBM Cloud . \"We are delighted to support Zyphra's strategic roadmap as we collaborate with AMD to deliver scalable, economical AI infrastructure that can accelerate Zyphra's model training.\" \"The IBM and AMD collaboration delivers innovation at the speed and scale clients demand, representing a new standard in AI infrastructure,\" said Philip Guido , EVP and Chief Commercial Officer, AMD . \"By combining IBM enterprise cloud expertise with AMD leadership in high-performance computing and AI acceleration, we are supporting Zyphra's pioneering work in multimodal and inference-efficient AI, enabling organizations everywhere to build smarter businesses and unlock AI solutions that drive real-world outcomes.\" IBM and AMD are forging new ground in AI infrastructure, helping redefine performance, efficiency, and scale for enterprise and startup customers. Their hybrid infrastructure environment offers a foundation for scaling AI, with options such as hybrid multi-cloud that can help boost cloud ROI and value for clients' generative AI deployments. IBM and AMD also recently announced plans to develop next-generation computing architectures, known as quantum-centric supercomputing, leveraging IBM's leadership in developing the world's most performant quantum computers and software, and AMD's leadership in high-performance computing and AI accelerators. For more information about the collaboration between IBM Cloud and AMD, please visit: https://www.ibm.com/products/gpu-ai-accelerator/amd IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain a competitive edge in their industries. Thousands of governments and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. Visit www.ibm.com for more information. For more than 55 years AMD has driven innovation in high-performance computing, graphics and visualization technologies. Billions of people, leading Fortune 500 businesses and cutting edge scientific research institutions around the world rely on AMD technology daily to improve how they live, work and play. AMD employees are focused on building leadership high- 3 performance and adaptive products that push the boundaries of what is possible. For more information about how AMD is enabling today and inspiring tomorrow, visit the AMD (NASDAQ: AMD) website , blog , LinkedIn , Facebook and X pages. Media Contact Kate Gazzillo kate.gazzillo@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "HashiCorp Previews the Future of Agentic Infrastructure Automation with Project infragraph",
      "description": "New Infrastructure and Security Lifecycle capabilities simplify hybrid operations and help move toward intelligent infrastructure operations",
      "url": "https://newsroom.ibm.com/2025-09-25-hashicorp-previews-the-future-of-agentic-infrastructure-automation-with-project-infragraph",
      "source": "IBM Think",
      "published_date": "2025-09-25T16:01:00",
      "author": null,
      "content": "SAN FRANCISCO , Sept. 25, 2025 / PRNewswire / -- Today at HashiConf 2025, the 10th global conference hosted by HashiCorp and its first as an IBM (NYSE: IBM ) company, the company introduced a series of Infrastructure and Security Lifecycle innovations, and a preview of Project infragraph — a new strategic investment for the HashiCorp Cloud Platform (HCP) that lays the groundwork for agentic infrastructure. Infrastructure as code and identity-based security are typically foundational practices for cloud programs. But complexity continues to grow as organizations work to operationalize AI, and infrastructure can require more intelligence, integration, and autonomous operations. These announcements reflect this shift, built to advance the capabilities needed to operate efficiently today, while helping teams prepare for agentic workflows. Project infragraph from HashiCorp, an IBM company, is a real-time infrastructure graph that connects infrastructure, applications, services, and ownership. Introducing Project infragraph: The foundation for agentic infrastructure As part of IBM, HashiCorp is accelerating its vision to deliver a unified control plane that extends across the hybrid cloud to support organizations of all sizes, operating across cloud environments. Modern enterprises lack a unified system of record for infrastructure and security. Visibility can be fragmented, context lost, and Day 2 operations suffer. Project infragraph looks to solve these challenges, as a real-time infrastructure graph that connects infrastructure, applications, services, and ownership. Project infragraph is planned to be delivered as a capability within the HashiCorp Cloud Platform (HCP). In the future, Project infragraph plans to extend HCP to connect to IBM's broader software portfolio, including Red Hat Ansible and OpenShift, IBM watsonx Orchestrate, Concert, Turbonomic, and Cloudability. This approach will help customers unify infrastructure, security, and applications under a consistent data and policy model. With Project infragraph, infrastructure teams can begin solving long-standing challenges around visibility, ownership, and data governance—without the complexity of fragmented tooling. The vision of Project infragraph is that over time, as more capabilities are added, the same graph will enable AI to reason about infrastructure state, propose runbooks and configuration changes, and effectively act across the application lifecycle. HashiCorp is now accepting applications for the private beta program for Project infragraph, which is expected to open in December 2025 . From Day 0 to Day N: What's new in ILM and SLM Key Infrastructure Lifecycle Management (ILM) and Security Lifecycle Management (SLM) updates demonstrate how HashiCorp is helping teams address today's infrastructure and security challenges—while advancing towards intelligent operations. Infrastructure Lifecycle Management (ILM) New ILM capabilities focus on making infrastructure provisioning, policy governance, and Day 2 operations faster, and more scalable across complex, hybrid environments. Security Lifecycle Management (SLM) New SLM enhancements improve secrets detection, simplify secure access, and support policy governance for modern enterprise environments. \"HashiCorp's latest product updates and the introduction of Project infragraph signal more than product momentum—they represent the evolution of a platform that can unify infrastructure and security data, and accelerate intelligent decision-making,\" said Armon Dadgar , CTO and co-founder of HashiCorp. \"We're focused on helping customers build secured, scalable cloud programs that are ready for AI and drive value to every stakeholder.\" \"Project infragraph is a major step toward infrastructure that can observe, reason, and act,\" said Dinesh Nirmal , Senior Vice President, IBM Software. \"By combining automation with real-time infrastructure intelligence, we are creating the control layer that unlocks the next era of AI-powered operations.\" Information about HashiConf 2025 HashiConf is HashiCorp's global cloud conference, featuring 2+ days of conversations on the future of cloud automation with product announcements, technical sessions, hands-on labs, certifications, social events, and more. HashiConf 2025 is sponsored by AWS, Microsoft, Arrow, Atyeti, Coder, Clumio, Datadog, Gomboc, Google Cloud, Mondoo, Overmind, Palo Alto Networks, Red Hat, River Point Technology, TD Synnex, and Wiz. To register for a free virtual pass to HashiConf — with access to a dedicated platform to view the live-streamed keynotes, educational content, and live chat with online attendees, as well as access to all virtual sessions on demand after the event — visit the conference website . All product announcements are available as referenced above, with more details available at hashicorp.com . Organizations interested in shaping the future of agentic infrastructure automation are invited to apply for the Project infragraph private beta . IBM's statements regarding future directions and intentions are subject to change or withdrawal without notice and represent goals and objectives only. HashiCorp, an IBM company, helps organizations automate hybrid cloud environments with Infrastructure and Security Lifecycle Management. HashiCorp offers The Infrastructure Cloud on the HashiCorp Cloud Platform (HCP) for managed cloud services, as well as self-hosted enterprise offerings and community source-available products. For more information, visit hashicorp.com. All product and company names are trademarks or registered trademarks of their respective holders. Red Hat, the Red Hat logo, OpenShift and Ansible are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries. Media & Analyst Contact: IBM Matt Marcus matt.marcus@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "New IBM watsonx AI-Powered Insights Help Elevate ESPN Fantasy Football for 2025 Fantasy Football Season",
      "description": "",
      "url": "https://newsroom.ibm.com/2025-09-24-new-ibm-watsonx-ai-powered-insights-help-elevate-espn-fantasy-football-for-2025-fantasy-football-season",
      "source": "IBM Think",
      "published_date": "2025-09-24T13:08:00",
      "author": null,
      "content": "ARMONK, N.Y. , Sept. 24, 2025 / PRNewswire / -- IBM (NYSE: IBM ) and ESPN are once again teaming up to enhance and improve the fantasy football experience with AI technology from IBM's watsonx. This season, the ESPN Fantasy Football app debuts the new weekly Fantasy Insights Built with IBM watsonx , giving the platform's more than 14 million fantasy football players personalized, AI-powered recommendations derived from over 36 billion insights throughout the season. Fantasy Insights Built With IBM watsonx appears on the home page of ESPN Fantasy Football, giving fantasy managers a quick, AI-generated overview of players that are of particular interest on any given week. It uses a set of series of data-driven categories, each with specific criteria, to identify those players. These build on the suite of AI-powered IBM Player Insights , which include Waiver Grades, Trade Grades, Boom and Bust probabilities, and the Trade Analyzer, all designed to transform massive volumes and varieties of data into actionable insights for fantasy managers. \"Fantasy football is all about optimizing decision making, and AI is helping fans make them with deeper insights,\" said Jonathan Adashek , Senior Vice President, Marketing and Communications, IBM . \"With Fantasy Insights Built With IBM watsonx, we're putting the power of watsonx into the hands of millions of people on the ESPN Fantasy Football app, helping them make smarter, more informed decisions while also showcasing the same capabilities businesses around the world use to scale AI.\" A series of categories within IBM Player Insights build on \"boom/bust\" and other data points, then aggregate them into actionable insights helping fantasy managers make more informed roster decisions. Notable for 2025: The feature uses AI models built with watsonx to identify players across several other categories, including: Together, IBM and ESPN enhance the fantasy football experience while also helping ESPN drive engagement across their platforms. ESPN uses watsonx.data to centralize data from disparate sources, preparing it for analysis by AI models to deliver unique, fan-focused features. To play ESPN Fantasy Football, sign up at ESPN.com/FFL or download the ESPN Fantasy App from the App Store and Android stores. IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain the competitive edge in their industries. Thousands of government and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. Visit www.ibm.com for more information. Media Contact Paul Pettas PaulPettas@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "SCREEN and IBM Sign Agreement for Next-Generation EUV Lithography Cleaning Process Development",
      "description": "Agreement builds on more than a decade of collaboration between the two companies",
      "url": "https://newsroom.ibm.com/2025-09-24-screen-and-ibm-sign-agreement-for-next-generation-euv-lithography-cleaning-process-development",
      "source": "IBM Think",
      "published_date": "2025-09-24T10:54:00",
      "author": null,
      "content": "ALBANY, NY and KYOTO, JAPAN, September 24 , 2025 – SCREEN Semiconductor Solutions Co., Ltd. and IBM (NYSE: IBM ) today announced an agreement to develop cleaning processes for next-generation EUV lithography. This agreement builds on a previous joint development collaboration for innovative cleaning processes that enabled the current generation of nanosheet device technology. In recent years, the adoption of EUV (Extreme Ultraviolet) lithography has been accelerating to meet the growing demand for miniaturization in advanced semiconductor manufacturing processes. In particular, High NA (High Numerical Aperture) EUV, a next-generation exposure technology, is gaining attention as an essential technology beyond the 2nm node. In these advanced EUV exposure processes, even minute particles or scratches on the wafer—previously considered negligible—can negatively impact patterning performance, making the cleaning process more critical than ever. To address these challenges, the two companies have signed an agreement to focus on developing cleaning technologies for High NA EUV. The collaboration combines IBM’s expertise in semiconductor process integration and SCREEN’s leading-edge wafer cleaning tools. \"High NA EUV technology is critical as we look to develop smaller, more powerful semiconductors for the age of AI,” said Mukesh Khare, GM of IBM Semiconductors and VP of Hybrid Cloud, IBM . “We are thrilled to expand our collaboration with SCREEN to ensure that IBM and our ecosystem partners can benefit from this technology innovation.” \"SCREEN is excited to deepen our collaboration with IBM to develop cleaning technologies that meet the stringent demands of High NA EUV lithography. By combining SCREEN precision cleaning expertise with IBM’s full stack development flow, we aim to deliver robust solutions that enable our customers to realize the potential of sub-2nm manufacturing,\" said Akihiko Okamoto, Representative Director and President of SCREEN Semiconductor Solutions . Through this joint development agreement, IBM and SCREEN will work to further accelerate the development of cleaning technologies for advanced semiconductor manufacturing using High NA EUV lithography and to provide solutions that maximize added value for device manufacturers. SCREEN Semiconductor Solutions Co., Ltd. is a leading manufacturer of wafer processing equipment for the global semiconductor market. We consistently hold the No.1 global share* in wafer cleaning equipment and deliver a wide range of solutions that underpin semiconductor production, including lithography, annealing, measurement/inspection systems. SCREEN offers an extensive lineup, from our 300 mm flagship models that deliver superior performance and productivity for the cutting-edge device market to the systems tuned for the IoT devices that handle substrates of various shapes and sizes of 200 mm or smaller. * Based on SCREEN in-house research IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain the competitive edge in their industries. Thousands of governments and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting delivers open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. SCREEN Semiconductor Solutions Co., Ltd. screenspe-info@screen.co.jp Willa Hahn IBM Communications Willa.hahn@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "IBM and BharatGen Collaborate to Accelerate AI Adoption in India Powered by Indic Large Language Models",
      "description": "",
      "url": "https://newsroom.ibm.com/2025-09-17-ibm-and-bharatgen-collaborate-to-accelerate-ai-adoption-in-India-powered-by-Indic-large-language-models",
      "source": "IBM Think",
      "published_date": "2025-09-17T16:24:00",
      "author": null,
      "content": "Mumbai – September 17, 2025 – IBM (NYSE: IBM ) and BharatGen today announced a strategic collaboration to advance the adoption of Artificial Intelligence (AI) in India powered by BharatGen’s sovereign multimodal and Large Language Models (LLMs) tailored to India’s unique linguistic and cultural landscape. This collaboration aims to bring together IBM's AI expertise in data, governance and model training technology, and BharatGen’s national mandate and expertise to create inclusive, India-centric sovereign multimodal and Large Language Models (LLMs) rooted in indigenous context and values. The initiative focuses on developing and scaling multimodal and language specific AI models and expanding their applications across various sectors, including education, agriculture, banking healthcare, citizen services and more. As part of this collaboration, BharatGen and IBM will aim to: \"At BharatGen, we have been building sovereign AI models and the ecosystem that reflects the linguistic richness, cultural nuances, and diverse needs of our people. This collaboration with IBM allows us to bring cutting-edge global research, scalable architectures and inclusive systems for India” said Prof. Gan esh Ramakrishnan , BharatGen. “With IBM’s strength in enterprise-grade platforms and our commitment to public-good AI, we are on a path to drive transformative solutions for empowering India’s digital journey across domains such as agriculture, finance, education, and governance.” “At IBM, we are committed to support the creation of open, trusted AI that solves real-world problems,” said Sandip Patel, Managing Director, IBM India and South Asia. “Through our collaboration with BharatGen, we aim to advance sovereign AI capabilities that reflect India’s diversity and deliver meaningful impact across sectors.” Standing from left to right: Ramesh Karwani, Head- Technology Policy, Global Regulatory Affairs, IBM India; Jaikrishnan Hari, Strategy and Business Development, IBM Research India; Dr. Amith Singhee, Director, IBM Research India; Sandip Patel, Managing Director, IBM India and South Asia; Shri Abhay Karandikar, Secretary, Department of Science and Technology (DST), Government of India; Prof Ganesh Ramakrishnan, Principal Investigator, BharatGen; Dr. Ekta Kapoor, Scientist- G and Head - Frontier and Futuristic Technologies (FFT) Division, DST; Prof Aditya Maheshwari, IIM Indore and Consortium Member, BharatGen, and other officials from DST. BharatGen’s LLM and foundation model roadmap is designed to address both national and commercial needs across agriculture, education, healthcare, national security, and finance. A key priority is the inclusion of underserved Indian languages and dialects beyond the top 12–22, ensuring broader digital participation and equity. BharatGen is a consortium under the Technology Innovation Hub at IIT Bombay and an Indian Government-Funded multimodal and large language model initiative for Indian languages, supported by the Department of Science and Technology (DST). Its mandate is to build AI for the nation by developing efficient AI models for Indian languages, creating a multilingual data repository, fostering public-private partnerships for scalable AI, and strengthening India's AI talent pool and startup ecosystem. BharatGen is building a family of LLMs and multi-modal FMs to serve diverse needs in India, including representing underserved languages, creating sovereign models for self-reliance, and partnering to build solutions for national importance across various sectors. IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain the competitive edge in their industries. Thousands of government and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. Visit www.ibm.com for more information. Prasanna Ramanathan Prasanna.S.R@ibm.com Antonetta Kumar antonkum@in.ibm.com Media Team press@bharatgen.com Sonia Kaushal Sonia.kaushal@qmettech.com",
      "category": "AI/Tech"
    },
    {
      "title": "Responding to the climate impact of generative AI",
      "description": "Explosive growth of AI data centers is expected to increase greenhouse gas emissions. Researchers are now seeking solutions to reduce these environmental harms.",
      "url": "https://news.mit.edu/2025/responding-to-generative-ai-climate-impact-0930",
      "source": "MIT News AI",
      "published_date": "2025-09-30T04:00:00",
      "author": "Adam Zewe | MIT News",
      "content": "Previous image Next image In part 2 of our two-part series on generative artificial intelligence’s environmental impacts , MIT News explores some of the ways experts are working to reduce the technology’s carbon footprint. The energy demands of generative AI are expected to continue increasing dramatically over the next decade. For instance, an April 2025 report from the International Energy Agency predicts that the global electricity demand from data centers , which house the computing infrastructure to train and deploy AI models, will more than double by 2030, to around 945 terawatt-hours. While not all operations performed in a data center are AI-related, this total amount is slightly more than the energy consumption of Japan. Moreover, an August 2025 analysis from Goldman Sachs Research forecasts that about 60 percent of the increasing electricity demands from data centers will be met by burning fossil fuels, increasing global carbon emissions by about 220 million tons . In comparison, driving a gas-powered car for 5,000 miles produces about 1 ton of carbon dioxide. These statistics are staggering, but at the same time, scientists and engineers at MIT and around the world are studying innovations and interventions to mitigate AI’s ballooning carbon footprint, from boosting the efficiency of algorithms to rethinking the design of data centers. Considering carbon emissions Talk of reducing generative AI’s carbon footprint is typically centered on “operational carbon” — the emissions used by the powerful processors, known as GPUs, inside a data center. It often ignores “embodied carbon,” which are emissions created by building the data center in the first place, says Vijay Gadepally, senior scientist at MIT Lincoln Laboratory, who leads research projects in the Lincoln Laboratory Supercomputing Center. Constructing and retrofitting a data center, built from tons of steel and concrete and filled with air conditioning units, computing hardware, and miles of cable, consumes a huge amount of carbon. In fact, the environmental impact of building data centers is one reason companies like Meta and Google are exploring more sustainable building materials. (Cost is another factor.) Plus, data centers are enormous buildings — the world’s largest, the China Telecomm-Inner Mongolia Information Park, engulfs roughly 10 million square feet — with about 10 to 50 times the energy density of a normal office building, Gadepally adds. “The operational side is only part of the story. Some things we are working on to reduce operational emissions may lend themselves to reducing embodied carbon, too, but we need to do more on that front in the future,” he says. Reducing operational carbon emissions When it comes to reducing operational carbon emissions of AI data centers, there are many parallels with home energy-saving measures. For one, we can simply turn down the lights. “Even if you have the worst lightbulbs in your house from an efficiency standpoint, turning them off or dimming them will always use less energy than leaving them running at full blast,” Gadepally says. In the same fashion, research from the Supercomputing Center has shown that “turning down” the GPUs in a data center so they consume about three-tenths the energy has minimal impacts on the performance of AI models, while also making the hardware easier to cool. Another strategy is to use less energy-intensive computing hardware. Demanding generative AI workloads, such as training new reasoning models like GPT-5, usually need many GPUs working simultaneously. The Goldman Sachs analysis estimates that a state-of-the-art system could soon have as many as 576 connected GPUs operating at once. But engineers can sometimes achieve similar results by reducing the precision of computing hardware, perhaps by switching to less powerful processors that have been tuned to handle a specific AI workload. There are also measures that boost the efficiency of training power-hungry deep-learning models before they are deployed. Gadepally’s group found that about half the electricity used for training an AI model is spent to get the last 2 or 3 percentage points in accuracy. Stopping the training process early can save a lot of that energy. “There might be cases where 70 percent accuracy is good enough for one particular application, like a recommender system for e-commerce,” he says. Researchers can also take advantage of efficiency-boosting measures. For instance, a postdoc in the Supercomputing Center realized the group might run a thousand simulations during the training process to pick the two or three best AI models for their project. By building a tool that allowed them to avoid about 80 percent of those wasted computing cycles, they dramatically reduced the energy demands of training with no reduction in model accuracy, Gadepally says. Leveraging efficiency improvements Constant innovation in computing hardware, such as denser arrays of transistors on semiconductor chips, is still enabling dramatic improvements in the energy efficiency of AI models. Even though energy efficiency improvements have been slowing for most chips since about 2005, the amount of computation that GPUs can do per joule of energy has been improving by 50 to 60 percent each year, says Neil Thompson, director of the FutureTech Research Project at MIT’s Computer Science and Artificial Intelligence Laboratory and a principal investigator at MIT’s Initiative on the Digital Economy. “The still-ongoing ‘Moore’s Law’ trend of getting more and more transistors on chip still matters for a lot of these AI systems, since running operations in parallel is still very valuable for improving efficiency,” says Thomspon. Even more significant, his group’s research indicates that efficiency gains from new model architectures that can solve complex problems faster, consuming less energy to achieve the same or better results, is doubling every eight or nine months. Thompson coined the term “ negaflop ” to describe this effect. The same way a “negawatt” represents electricity saved due to energy-saving measures, a “negaflop” is a computing operation that doesn’t need to be performed due to algorithmic improvements. These could be things like “ pruning ” away unnecessary components of a neural network or employing compression techniques that enable users to do more with less computation. “If you need to use a really powerful model today to complete your task, in just a few years, you might be able to use a significantly smaller model to do the same thing, which would carry much less environmental burden. Making these models more efficient is the single-most important thing you can do to reduce the environmental costs of AI,” Thompson says. Maximizing energy savings While reducing the overall energy use of AI algorithms and computing hardware will cut greenhouse gas emissions, not all energy is the same, Gadepally adds. “The amount of carbon emissions in 1 kilowatt hour varies quite significantly, even just during the day, as well as over the month and year,” he says. Engineers can take advantage of these variations by leveraging the flexibility of AI workloads and data center operations to maximize emissions reductions. For instance, some generative AI workloads don’t need to be performed in their entirety at the same time. Splitting computing operations so some are performed later, when more of the electricity fed into the grid is from renewable sources like solar and wind, can go a long way toward reducing a data center’s carbon footprint, says Deepjyoti Deka, a research scientist in the MIT Energy Initiative. Deka and his team are also studying “smarter” data centers where the AI workloads of multiple companies using the same computing equipment are flexibly adjusted to improve energy efficiency. “By looking at the system as a whole, our hope is to minimize energy use as well as dependence on fossil fuels, while still maintaining reliability standards for AI companies and users,” Deka says. He and others at MITEI are building a flexibility model of a data center that considers the differing energy demands of training a deep-learning model versus deploying that model. Their hope is to uncover the best strategies for scheduling and streamlining computing operations to improve energy efficiency. The researchers are also exploring the use of long-duration energy storage units at data centers, which store excess energy for times when it is needed. With these systems in place, a data center could use stored energy that was generated by renewable sources during a high-demand period, or avoid the use of diesel backup generators if there are fluctuations in the grid. “Long-duration energy storage could be a game-changer here because we can design operations that really change the emission mix of the system to rely more on renewable energy,” Deka says. In addition, researchers at MIT and Princeton University are developing a software tool for investment planning in the power sector, called GenX , which could be used to help companies determine the ideal place to locate a data center to minimize environmental impacts and costs. Location can have a big impact on reducing a data center’s carbon footprint. For instance, Meta operates a data center in Lulea , a city on the coast of northern Sweden where cooler temperatures reduce the amount of electricity needed to cool computing hardware. Thinking farther outside the box (way farther), some governments are even exploring the construction of data centers on the moon where they could potentially be operated with nearly all renewable energy. Currently, the expansion of renewable energy generation here on Earth isn’t keeping pace with the rapid growth of AI, which is one major roadblock to reducing its carbon footprint, says Jennifer Turliuk MBA ’25, a short-term lecturer, former Sloan Fellow, and former practice leader of climate and energy AI at the Martin Trust Center for MIT Entrepreneurship. The local, state, and federal review processes required for a new renewable energy projects can take years. Researchers at MIT and elsewhere are exploring the use of AI to speed up the process of connecting new renewable energy systems to the power grid. For instance, a generative AI model could streamline interconnection studies that determine how a new project will impact the power grid, a step that often takes years to complete. And when it comes to accelerating the development and implementation of clean energy technologies , AI could play a major role. “Machine learning is great for tackling complex situations, and the electrical grid is said to be one of the largest and most complex machines in the world,” Turliuk adds. For instance, AI could help optimize the prediction of solar and wind energy generation or identify ideal locations for new facilities. It could also be used to perform predictive maintenance and fault detection for solar panels or other green energy infrastructure, or to monitor the capacity of transmission wires to maximize efficiency. By helping researchers gather and analyze huge amounts of data, AI could also inform targeted policy interventions aimed at getting the biggest “bang for the buck” from areas such as renewable energy, Turliuk says. To help policymakers, scientists, and enterprises consider the multifaceted costs and benefits of AI systems, she and her collaborators developed the Net Climate Impact Score. The score is a framework that can be used to help determine the net climate impact of AI projects, considering emissions and other environmental costs along with potential environmental benefits in the future. At the end of the day, the most effective solutions will likely result from collaborations among companies, regulators, and researchers, with academia leading the way, Turliuk adds. “Every day counts. We are on a path where the effects of climate change won’t be fully known until it is too late to do anything about it. This is a once-in-a-lifetime opportunity to innovate and make AI systems less carbon-intense,” she says. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "AI system learns from many types of scientific information and runs experiments to discover new materials",
      "description": "The new “CRESt” platform could help find solutions to real-world energy problems that have plagued the materials science and engineering community for decades.",
      "url": "https://news.mit.edu/2025/ai-system-learns-many-types-scientific-information-and-runs-experiments-discovering-new-materials-0925",
      "source": "MIT News AI",
      "published_date": "2025-09-25T15:00:00",
      "author": "Zach Winn | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image Machine-learning models can speed up the discovery of new materials by making predictions and suggesting experiments. But most models today only consider a few specific types of data or variables. Compare that with human scientists, who work in a collaborative environment and consider experimental results, the broader scientific literature, imaging and structural analysis, personal experience or intuition, and input from colleagues and peer reviewers. Now, MIT researchers have developed a method for optimizing materials recipes and planning experiments that incorporates information from diverse sources like insights from the literature, chemical compositions, microstructural images, and more. The approach is part of a new platform, named Copilot for Real-world Experimental Scientists (CRESt), that also uses robotic equipment for high-throughput materials testing, the results of which are fed back into large multimodal models to further optimize materials recipes. Human researchers can converse with the system in natural language, with no coding required, and the system makes its own observations and hypotheses along the way. Cameras and visual language models also allow the system to monitor experiments, detect issues, and suggest corrections. “In the field of AI for science, the key is designing new experiments,” says Ju Li, School of Engineering Carl Richard Soderberg Professor of Power Engineering. “We use multimodal feedback — for example information from previous literature on how palladium behaved in fuel cells at this temperature, and human feedback — to complement experimental data and design new experiments. We also use robots to synthesize and characterize the material’s structure and to test performance.” The system is described in a paper published in Nature . The researchers used CRESt to explore more than 900 chemistries and conduct 3,500 electrochemical tests, leading to the discovery of a catalyst material that delivered record power density in a fuel cell that runs on formate salt to produce electricity. Joining Li on the paper as first authors are PhD student Zhen Zhang, Zhichu Ren PhD ’24, PhD student Chia-Wei Hsu, and postdoc Weibin Chen. Their coauthors are MIT Assistant Professor Iwnetim Abate; Associate Professor Pulkit Agrawal; JR East Professor of Engineering Yang Shao-Horn; MIT.nano researcher Aubrey Penn; Zhang-Wei Hong PhD ’25, Hongbin Xu PhD ’25; Daniel Zheng PhD ’25; MIT graduate students Shuhan Miao and Hugh Smith; MIT postdocs Yimeng Huang, Weiyin Chen, Yungsheng Tian, Yifan Gao, and Yaoshen Niu; former MIT postdoc Sipei Li; and collaborators including Chi-Feng Lee, Yu-Cheng Shao, Hsiao-Tsu Wang, and Ying-Rui Lu. A smarter system Materials science experiments can be time-consuming and expensive. They require researchers to carefully design workflows, make new material, and run a series of tests and analysis to understand what happened. Those results are then used to decide how to improve the material. To improve the process, some researchers have turned to a machine-learning strategy known as active learning to make efficient use of previous experimental data points and explore or exploit those data. When paired with a statistical technique known as Bayesian optimization (BO), active learning has helped researchers identify new materials for things like batteries and advanced semiconductors. “Bayesian optimization is like Netflix recommending the next movie to watch based on your viewing history, except instead it recommends the next experiment to do,” Li explains. “But basic Bayesian optimization is too simplistic. It uses a boxed-in design space, so if I say I’m going to use platinum, palladium, and iron, it only changes the ratio of those elements in this small space. But real materials have a lot more dependencies, and BO often gets lost.” Most active learning approaches also rely on single data streams that don’t capture everything that goes on in an experiment. To equip computational systems with more human-like knowledge, while still taking advantage of the speed and control of automated systems, Li and his collaborators built CRESt. CRESt’s robotic equipment includes a liquid-handling robot, a carbothermal shock system to rapidly synthesize materials, an automated electrochemical workstation for testing, characterization equipment including automated electron microscopy and optical microscopy, and auxiliary devices such as pumps and gas valves, which can also be remotely controlled. Many processing parameters can also be tuned. With the user interface, researchers can chat with CRESt and tell it to use active learning to find promising materials recipes for different projects. CRESt can include up to 20 precursor molecules and substrates into its recipe. To guide material designs, CRESt’s models search through scientific papers for descriptions of elements or precursor molecules that might be useful. When human researchers tell CRESt to pursue new recipes, it kicks off a robotic symphony of sample preparation, characterization, and testing. The researcher can also ask CRESt to perform image analysis from scanning electron microscopy imaging, X-ray diffraction, and other sources. Information from those processes is used to train the active learning models, which use both literature knowledge and current experimental results to suggest further experiments and accelerate materials discovery. “For each recipe we use previous literature text or databases, and it creates these huge representations of every recipe based on the previous knowledge base before even doing the experiment,” says Li. “We perform principal component analysis in this knowledge embedding space to get a reduced search space that captures most of the performance variability. Then we use Bayesian optimization in this reduced space to design the new experiment. After the new experiment, we feed newly acquired multimodal experimental data and human feedback into a large language model to augment the knowledgebase and redefine the reduced search space, which gives us a big boost in active learning efficiency.” Materials science experiments can also face reproducibility challenges. To address the problem, CRESt monitors its experiments with cameras, looking for potential problems and suggesting solutions via text and voice to human researchers. The researchers used CRESt to develop an electrode material for an advanced type of high-density fuel cell known as a direct formate fuel cell. After exploring more than 900 chemistries over three months, CRESt discovered a catalyst material made from eight elements that achieved a 9.3-fold improvement in power density per dollar over pure palladium, an expensive precious metal. In further tests, CRESTs material was used to deliver a record power density to a working direct formate fuel cell even though the cell contained just one-fourth of the precious metals of previous devices. The results show the potential for CRESt to find solutions to real-world energy problems that have plagued the materials science and engineering community for decades. “A significant challenge for fuel-cell catalysts is the use of precious metal,” says Zhang. “For fuel cells, researchers have used various precious metals like palladium and platinum. We used a multielement catalyst that also incorporates many other cheap elements to create the optimal coordination environment for catalytic activity and resistance to poisoning species such as carbon monoxide and adsorbed hydrogen atom. People have been searching low-cost options for many years. This system greatly accelerated our search for these catalysts.” A helpful assistant Early on, poor reproducibility emerged as a major problem that limited the researchers’ ability to perform their new active learning technique on experimental datasets. Material properties can be influenced by the way the precursors are mixed and processed, and any number of problems can subtly alter experimental conditions, requiring careful inspection to correct. To partially automate the process, the researchers coupled computer vision and vision language models with domain knowledge from the scientific literature, which allowed the system to hypothesize sources of irreproducibility and propose solutions. For example, the models can notice when there’s a millimeter-sized deviation in a sample’s shape or when a pipette moves something out of place. The researchers incorporated some of the model’s suggestions, leading to improved consistency, suggesting the models already make good experimental assistants. The researchers noted that humans still performed most of the debugging in their experiments. “CREST is an assistant, not a replacement, for human researchers,” Li says. “Human researchers are still indispensable. In fact, we use natural language so the system can explain what it is doing and present observations and hypotheses. But this is a step toward more flexible, self-driving labs.” Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "New AI system could accelerate clinical research",
      "description": "By enabling rapid annotation of areas of interest in medical images, the tool can help scientists study new treatments or map disease progression.",
      "url": "https://news.mit.edu/2025/new-ai-system-could-accelerate-clinical-research-0925",
      "source": "MIT News AI",
      "published_date": "2025-09-25T04:00:00",
      "author": "Adam Zewe | MIT News",
      "content": "Previous image Next image Annotating regions of interest in medical images, a process known as segmentation, is often one of the first steps clinical researchers take when running a new study involving biomedical images. For instance, to determine how the size of the brain’s hippocampus changes as patients age, the scientist first outlines each hippocampus in a series of brain scans. For many structures and image types, this is often a manual process that can be extremely time-consuming, especially if the regions being studied are challenging to delineate. To streamline the process, MIT researchers developed an artificial intelligence-based system that enables a researcher to rapidly segment new biomedical imaging datasets by clicking, scribbling, and drawing boxes on the images. This new AI model uses these interactions to predict the segmentation. As the user marks additional images, the number of interactions they need to perform decreases, eventually dropping to zero. The model can then segment each new image accurately without user input. It can do this because the model’s architecture has been specially designed to use information from images it has already segmented to make new predictions. Unlike other medical image segmentation models, this system allows the user to segment an entire dataset without repeating their work for each image. In addition, the interactive tool does not require a presegmented image dataset for training, so users don’t need machine-learning expertise or extensive computational resources. They can use the system for a new segmentation task without retraining the model. In the long run, this tool could accelerate studies of new treatment methods and reduce the cost of clinical trials and medical research. It could also be used by physicians to improve the efficiency of clinical applications, such as radiation treatment planning. “Many scientists might only have time to segment a few images per day for their research because manual image segmentation is so time-consuming. Our hope is that this system will enable new science by allowing clinical researchers to conduct studies they were prohibited from doing before because of the lack of an efficient tool,” says Hallee Wong, an electrical engineering and computer science graduate student and lead author of a paper on this new tool . She is joined on the paper by Jose Javier Gonzalez Ortiz PhD ’24; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering; and senior author Adrian Dalca, an assistant professor at Harvard Medical School and MGH, and a research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference on Computer Vision. There are primarily two methods researchers use to segment new sets of medical images. With interactive segmentation, they input an image into an AI system and use an interface to mark areas of interest. The model predicts the segmentation based on those interactions. A tool previously developed by the MIT researchers, ScribblePrompt , allows users to do this, but they must repeat the process for each new image. Another approach is to develop a task-specific AI model to automatically segment the images. This approach requires the user to manually segment hundreds of images to create a dataset, and then train a machine-learning model. That model predicts the segmentation for a new image. But the user must start the complex, machine-learning-based process from scratch for each new task, and there is no way to correct the model if it makes a mistake. This new system, MultiverSeg , combines the best of each approach. It predicts a segmentation for a new image based on user interactions, like scribbles, but also keeps each segmented image in a context set that it refers to later. When the user uploads a new image and marks areas of interest, the model draws on the examples in its context set to make a more accurate prediction, with less user input. The researchers designed the model’s architecture to use a context set of any size, so the user doesn’t need to have a certain number of images. This gives MultiverSeg the flexibility to be used in a range of applications. “At some point, for many tasks, you shouldn’t need to provide any interactions. If you have enough examples in the context set, the model can accurately predict the segmentation on its own,” Wong says. The researchers carefully engineered and trained the model on a diverse collection of biomedical imaging data to ensure it had the ability to incrementally improve its predictions based on user input. The user doesn’t need to retrain or customize the model for their data. To use MultiverSeg for a new task, one can upload a new medical image and start marking it. When the researchers compared MultiverSeg to state-of-the-art tools for in-context and interactive image segmentation, it outperformed each baseline. Fewer clicks, better results Unlike these other tools, MultiverSeg requires less user input with each image. By the ninth new image, it needed only two clicks from the user to generate a segmentation more accurate than a model designed specifically for the task. For some image types, like X-rays, the user might only need to segment one or two images manually before the model becomes accurate enough to make predictions on its own. The tool’s interactivity also enables the user to make corrections to the model’s prediction, iterating until it reaches the desired level of accuracy. Compared to the researchers’ previous system, MultiverSeg reached 90 percent accuracy with roughly 2/3 the number of scribbles and 3/4 the number of clicks. “With MultiverSeg, users can always provide more interactions to refine the AI predictions. This still dramatically accelerates the process because it is usually faster to correct something that exists than to start from scratch,” Wong says. Moving forward, the researchers want to test this tool in real-world situations with clinical collaborators and improve it based on user feedback. They also want to enable MultiverSeg to segment 3D biomedical images. This work is supported, in part, by Quanta Computer, Inc. and the National Institutes of Health, with hardware support from the Massachusetts Life Sciences Center. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Improving the workplace of the future",
      "description": "Economics doctoral student Whitney Zhang investigates how technologies and organizational decisions shape labor markets.",
      "url": "https://news.mit.edu/2025/improving-workplace-future-whitney-zhang-0924",
      "source": "MIT News AI",
      "published_date": "2025-09-24T04:00:00",
      "author": "Benjamin Daniel | School of Humanities, Arts, and Social Sciences",
      "content": "Previous image Next image Whitney Zhang ’21 believes in the importance of valuing workers regardless of where they fit into an organizational chart. Zhang is a PhD student in MIT’s Department of Economics studying labor economics. She explores how the technological and managerial decisions companies make affect workers across the pay spectrum. “I’ve been interested in economics, economic impacts, and related social issues for a long time,” says Zhang, who majored in mathematical economics as an undergraduate. “I wanted to apply my math skills to see how we could improve policies and their effects.” Zhang is interested in how to improve conditions for workers. She believes it’s important to build relationships with policymakers, focusing on an evidence-driven approach to policy, while always remembering to center those the policies may affect. “We have to remember the people whose lives are impacted by business operations and legislation,” she says. She’s also aware of the complex intermixture of politics, social status, and financial obligations organizations and their employees have to navigate. “Though I’m studying workers, it’s important to consider the entire complex ecosystem when solving for these kinds of challenges, including firm incentives and global economic conditions,” she says. The intersection of tech and labor policy Zhang began investigating employee productivity, artificial intelligence, and related economic and labor market phenomena early in her time as a doctoral student, collaborating frequently with fellow PhD students in the department. A collaboration with economics doctoral student Shakked Noy yielded the 2023 study investigating ChatGPT as a tool to improve productivity. Their research found it substantially increased workers’ productivity on writing tasks, most so for workers who initially performed the worst on the tasks. “This was one of the earliest pieces of evidence on the productivity effects of generative AI, and contributed to providing concrete data on how impactful these types of tools might be in the workplace and on the labor market,” Zhang says. In other ongoing research — “Determinants of Irregular Worker Schedules” — Zhang is using data from a payroll provider to examine scheduling unpredictability, investigating why companies employ unpredictable schedules and how these schedules affect low-wage employees’ quality of life. The scheduling project, conducted with MIT economics PhD student Nathan Lazarus, is motivated, in part, by existing sociological evidence that low-wage workers’ unpredictable schedules are associated with worse sleep and well-being. “We’ve seen a relationship between higher turnover and inconsistent, inadequate schedules, which suggests workers dis-prefer these kinds of schedules,” Zhang says. At an academic roundtable, Zhang presented her results to Starbucks employees involved in scheduling and staffing. The attendees wanted to learn more about how different scheduling practices impacted workers and their productivity. “These are the kinds of questions that could reveal useful information for small businesses, large corporations, and others,” she says. By conducting this research, Zhang hopes to better understand whether or not scheduling regulations can improve affected employees’ quality of life, while also considering potential unintended consequences. “Why are these schedules set the way they’re set?” she asks. “Do businesses with these kinds of schedules require increased regulation?” Another project, conducted with MIT economics doctoral student Arjun Ramani, examines the linkages between offshoring, remote work, and related outcomes. “Do the technological and managerial practices that have made remote work possible further facilitate offshoring?” she asks. “Do organizations see significant gains in efficiency? What are the impacts on U.S. and offshore workers?” Her work is being funded through the National Science Foundation Graduate Research Fellowship Program and the Washington Center for Equitable Growth . Putting people at the center Zhang has observed the different kinds of people economics and higher education could bring together. She followed a dual enrollment track in high school, completing college-level courses with students from across a variety of demographic identities. “I enjoyed centering people in my work,” she says. “Taking classes with a diverse group of students, including veterans and mothers returning to school to complete their studies, made me more curious about socioeconomic issues and the policies relevant to them.” She later enrolled at MIT, where she participated in the Undergraduate Research Opportunities Program ( UROP ). She also completed an internship at the World Bank, worked as a summer analyst at the Federal Reserve Bank of New York, and worked as an assistant for a diverse faculty cohort including MIT economists David Autor , Jon Gruber , and Nina Roussille . Autor is her primary advisor on her doctoral research, a mentor she cites as a significant influence. “[Autor’s] course, 14.03 (Microeconomics and Public Policy), cemented connections between theory and practice,” she says. “I thought the class was revelatory in showing the kinds of questions economics can answer.” Doctoral study has revealed interesting pathways of investigation for Zhang, as have her relationships with her student peers and other faculty. She has, for example, leveraged faculty connections to gain access to hourly wage data in support of her scheduling and employee impacts work. “Generally, economists have had administrative data on earnings, but not on hours,” she notes. Zhang’s focus on improving others’ lives extends to her work outside the classroom. She’s a mentor for the Boston Chinatown Neighborhood Center College Access Program and a member of MIT’s Graduate Christian Fellowship group. When she’s not enjoying spicy soups or paddling on the Charles, she takes advantage of opportunities to decompress with her art at W20 Arts Studios . “I wanted to create time for myself outside of research and the classroom,” she says. Zhang cites the benefits of MIT’s focus on cross-collaboration and encouraging students to explore other disciplines. As an undergraduate, Zhang minored in computer science, which taught her coding skills critical to her data work. Exposure to engineering also led her to become more interested in questions around how technology and workers interact. Working with other scholars in the department has improved how Zhang conducts inquiries. “I’ve become the kind of well-rounded student and professional who can identify and quantify impacts, which is invaluable for future projects,” she says. Exposure to different academic and research areas, Zhang argues, helps increase access to ideas and information. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "MIT affiliates win AI for Math grants to accelerate mathematical discovery",
      "description": "Department of Mathematics researchers David Roe and Andrew Sutherland seek to advance automated theorem proving; four additional MIT alumni also awarded.",
      "url": "https://news.mit.edu/2025/ai-for-math-grants-accelerate-mathematical-discovery-0922",
      "source": "MIT News AI",
      "published_date": "2025-09-22T19:15:00",
      "author": "Sandi Miller | Department of Mathematics",
      "content": "Previous image Next image MIT Department of Mathematics researchers David Roe ’06 and Andrew Sutherland ’90, PhD ’07 are among the inaugural recipients of the Renaissance Philanthropy and XTX Markets’ AI for Math grants . Four additional MIT alumni — Anshula Gandhi ’19, Viktor Kunčak SM ’01, PhD ’07; Gireeja Ranade ’07; and Damiano Testa PhD ’05 — were also honored for separate projects. The first 29 winning projects will support mathematicians and researchers at universities and organizations working to develop artificial intelligence systems that help advance mathematical discovery and research across several key tasks. Roe and Sutherland, along with Chris Birkbeck of the University of East Anglia, will use their grant to boost automated theorem proving by building connections between the L-Functions and Modular Forms Database (LMFDB) and the Lean4 mathematics library (mathlib). “Automated theorem provers are quite technically involved, but their development is under-resourced,” says Sutherland. With AI technologies such as large language models (LLMs), the barrier to entry for these formal tools is dropping rapidly, making formal verification frameworks accessible to working mathematicians. Mathlib is a large, community-driven mathematical library for the Lean theorem prover, a formal system that verifies the correctness of every step in a proof. Mathlib currently contains on the order of 10 5 mathematical results (such as lemmas, propositions, and theorems). The LMFDB, a massive, collaborative online resource that serves as a kind of “encyclopedia” of modern number theory, contains more than 10 9 concrete statements. Sutherland and Roe are managing editors of the LMFDB. Roe and Sutherland’s grant will be used for a project that aims to augment both systems, making the LMFDB’s results available within mathlib as assertions that have not yet been formally proved, and providing precise formal definitions of the numerical data stored within the LMFDB. This bridge will benefit both human mathematicians and AI agents, and provide a framework for connecting other mathematical databases to formal theorem-proving systems. The main obstacles to automating mathematical discovery and proof are the limited amount of formalized math knowledge, the high cost of formalizing complex results, and the gap between what is computationally accessible and what is feasible to formalize. To address these obstacles, the researchers will use the funding to build tools for accessing the LMFDB from mathlib, making a large database of unformalized mathematical knowledge accessible to a formal proof system. This approach enables proof assistants to identify specific targets for formalization without the need to formalize the entire LMFDB corpus in advance. “Making a large database of unformalized number-theoretic facts available within mathlib will provide a powerful technique for mathematical discovery, because the set of facts an agent might wish to consider while searching for a theorem or proof is exponentially larger than the set of facts that eventually need to be formalized in actually proving the theorem,” says Roe. The researchers note that proving new theorems at the frontier of mathematical knowledge often involves steps that rely on a nontrivial computation. For example, Andrew Wiles’ proof of Fermat’s Last Theorem uses what is known as the “3-5 trick” at a crucial point in the proof. “This trick depends on the fact that the modular curve X_0(15) has only finitely many rational points, and none of those rational points correspond to a semi-stable elliptic curve,” according to Sutherland. “This fact was known well before Wiles’ work, and is easy to verify using computational tools available in modern computer algebra systems, but it is not something one can realistically prove using pencil and paper, nor is it necessarily easy to formalize.” While formal theorem provers are being connected to computer algebra systems for more efficient verification, tapping into computational outputs in existing mathematical databases offers several other benefits. Using stored results leverages the thousands of CPU-years of computation time already spent in creating the LMFDB, saving money that would be needed to redo these computations. Having precomputed information available also makes it feasible to search for examples or counterexamples without knowing ahead of time how broad the search can be. In addition, mathematical databases are curated repositories, not simply a random collection of facts. “The fact that number theorists emphasized the role of the conductor in databases of elliptic curves has already proved to be crucial to one notable mathematical discovery made using machine learning tools: murmurations ,” says Sutherland. “Our next steps are to build a team, engage with both the LMFDB and mathlib communities, start to formalize the definitions that underpin the elliptic curve, number field, and modular form sections of the LMFDB, and make it possible to run LMFDB searches from within mathlib,” says Roe. “If you are an MIT student interested in getting involved, feel free to reach out!” Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "New tool makes generative AI models more likely to create breakthrough materials",
      "description": "With SCIGEN, researchers can steer AI models to create materials with exotic properties for applications like quantum computing.",
      "url": "https://news.mit.edu/2025/new-tool-makes-generative-ai-models-likely-create-breakthrough-materials-0922",
      "source": "MIT News AI",
      "published_date": "2025-09-22T09:00:00",
      "author": "Zach Winn | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image The artificial intelligence models that turn text into images are also useful for generating new materials. Over the last few years, generative materials models from companies like Google, Microsoft, and Meta have drawn on their training data to help researchers design tens of millions of new materials. But when it comes to designing materials with exotic quantum properties like superconductivity or unique magnetic states, those models struggle. That’s too bad, because humans could use the help. For example, after a decade of research into a class of materials that could revolutionize quantum computing, called quantum spin liquids, only a dozen material candidates have been identified. The bottleneck means there are fewer materials to serve as the basis for technological breakthroughs. Now, MIT researchers have developed a technique that lets popular generative materials models create promising quantum materials by following specific design rules. The rules, or constraints, steer models to create materials with unique structures that give rise to quantum properties. “The models from these large companies generate materials optimized for stability,” says Mingda Li, MIT’s Class of 1947 Career Development Professor. “Our perspective is that’s not usually how materials science advances. We don’t need 10 million new materials to change the world. We just need one really good material.” The approach is described today in a paper published by Nature Materials . The researchers applied their technique to generate millions of candidate materials consisting of geometric lattice structures associated with quantum properties. From that pool, they synthesized two actual materials with exotic magnetic traits. “People in the quantum community really care about these geometric constraints, like the Kagome lattices that are two overlapping, upside-down triangles. We created materials with Kagome lattices because those materials can mimic the behavior of rare earth elements, so they are of high technical importance.” Li says. Li is the senior author of the paper. His MIT co-authors include PhD students Ryotaro Okabe, Mouyang Cheng, Abhijatmedhi Chotrattanapituk, and Denisse Cordova Carrizales; postdoc Manasi Mandal; undergraduate researchers Kiran Mak and Bowen Yu; visiting scholar Nguyen Tuan Hung; Xiang Fu ’22, PhD ’24; and professor of electrical engineering and computer science Tommi Jaakkola, who is an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and Institute for Data, Systems, and Society. Additional co-authors include Yao Wang of Emory University, Weiwei Xie of Michigan State University, YQ Cheng of Oak Ridge National Laboratory, and Robert Cava of Princeton University. Steering models toward impact A material’s properties are determined by its structure, and quantum materials are no different. Certain atomic structures are more likely to give rise to exotic quantum properties than others. For instance, square lattices can serve as a platform for high-temperature superconductors, while other shapes known as Kagome and Lieb lattices can support the creation of materials that could be useful for quantum computing. To help a popular class of generative models known as a diffusion models produce materials that conform to particular geometric patterns, the researchers created SCIGEN (short for Structural Constraint Integration in GENerative model). SCIGEN is a computer code that ensures diffusion models adhere to user-defined constraints at each iterative generation step. With SCIGEN, users can give any generative AI diffusion model geometric structural rules to follow as it generates materials. AI diffusion models work by sampling from their training dataset to generate structures that reflect the distribution of structures found in the dataset. SCIGEN blocks generations that don’t align with the structural rules. To test SCIGEN, the researchers applied it to a popular AI materials generation model known as DiffCSP. They had the SCIGEN-equipped model generate materials with unique geometric patterns known as Archimedean lattices, which are collections of 2D lattice tilings of different polygons. Archimedean lattices can lead to a range of quantum phenomena and have been the focus of much research. “Archimedean lattices give rise to quantum spin liquids and so-called flat bands, which can mimic the properties of rare earths without rare earth elements, so they are extremely important,” says Cheng, a co-corresponding author of the work. “Other Archimedean lattice materials have large pores that could be used for carbon capture and other applications, so it’s a collection of special materials. In some cases, there are no known materials with that lattice, so I think it will be really interesting to find the first material that fits in that lattice.” The model generated over 10 million material candidates with Archimedean lattices. One million of those materials survived a screening for stability. Using the supercomputers in Oak Ridge National Laboratory, the researchers then took a smaller sample of 26,000 materials and ran detailed simulations to understand how the materials’ underlying atoms behaved. The researchers found magnetism in 41 percent of those structures. From that subset, the researchers synthesized two previously undiscovered compounds, TiPdBi and TiPbSb, at Xie and Cava’s labs. Subsequent experiments showed the AI model’s predictions largely aligned with the actual material’s properties. “We wanted to discover new materials that could have a huge potential impact by incorporating these structures that have been known to give rise to quantum properties,” says Okabe, the paper’s first author. “We already know that these materials with specific geometric patterns are interesting, so it’s natural to start with them.” Accelerating material breakthroughs Quantum spin liquids could unlock quantum computing by enabling stable, error-resistant qubits that serve as the basis of quantum operations. But no quantum spin liquid materials have been confirmed. Xie and Cava believe SCIGEN could accelerate the search for these materials. “There’s a big search for quantum computer materials and topological superconductors, and these are all related to the geometric patterns of materials,” Xie says. “But experimental progress has been very, very slow,” Cava adds. “Many of these quantum spin liquid materials are subject to constraints: They have to be in a triangular lattice or a Kagome lattice. If the materials satisfy those constraints, the quantum researchers get excited; it’s a necessary but not sufficient condition. So, by generating many, many materials like that, it immediately gives experimentalists hundreds or thousands more candidates to play with to accelerate quantum computer materials research.” “This work presents a new tool, leveraging machine learning, that can predict which materials will have specific elements in a desired geometric pattern,” says Drexel University Professor Steve May, who was not involved in the research. “This should speed up the development of previously unexplored materials for applications in next-generation electronic, magnetic, or optical technologies.” The researchers stress that experimentation is still critical to assess whether AI-generated materials can be synthesized and how their actual properties compare with model predictions. Future work on SCIGEN could incorporate additional design rules into generative models, including chemical and functional constraints. “People who want to change the world care about material properties more than the stability and structure of materials,” Okabe says. “With our approach, the ratio of stable materials goes down, but it opens the door to generate a whole bunch of promising materials.” The work was supported, in part, by the U.S. Department of Energy, the National Energy Research Scientific Computing Center, the National Science Foundation, and Oak Ridge National Laboratory. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "How are MIT entrepreneurs using AI?",
      "description": "This year’s delta v summer accelerator offered an up-close look at how AI is changing the process of building a startup.",
      "url": "https://news.mit.edu/2025/how-are-mit-entrepreneurs-using-ai-0922",
      "source": "MIT News AI",
      "published_date": "2025-09-22T04:00:00",
      "author": "Zach Winn | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image The Martin Trust Center for MIT Entrepreneurship strives to teach students the craft of entrepreneurship. Over the last few years, no technology has changed that craft more than artificial intelligence. While many are predicting a rapid and complete transformation in how startups are built, the Trust Center’s leaders have a more nuanced view. “The fundamentals of entrepreneurship haven’t changed with AI,” says Trust Center Entrepreneur in Residence Macauley Kenney. “There’s been a shift in how entrepreneurs accomplish tasks, and that trickles down into how you build a company, but we’re thinking of AI as another new tool in the toolkit. In some ways the world is moving a lot faster, but we also need to make sure the fundamental principles of entrepreneurship are well-understood.” That approach was on display during this summer’s delta v startup accelerator program, where many students regularly turned to AI tools but still ultimately relied on talking to their customers to make the right decisions for their business. Students in this year’s cohort used AI tools to accelerate their coding, draft presentations, learn about new industries, and brainstorm ideas. The Trust Center is encouraging students to use AI as they see fit while also staying mindful of the technology’s limitations. The Trust Center itself has also embraced AI, most notably through Jetpack, its generative AI app that walks users through the 24 steps of disciplined entrepreneurship outlined in Managing Director Bill Aulet’s book of the same name. When students input a startup idea, the tool can suggest customer segments, early markets to pursue, business models, pricing, and a product plan. The ways the Trust Center wants students to use Jetpack is apparent in its name: It’s inspired by the acceleration a jetpack provides, but users still need to guide its direction. Even with AI technology’s current limitations, the Trust Center’s leaders acknowledge it can be a powerful tool for people at any stage of building a business, and their use of AI will continue to evolve with the technology. “It’s undeniable we’re in the midst of an AI revolution right now,” says Entrepreneur in Residence Ben Soltoff. “AI is reshaping a lot of things we do, and it’s also shaping how we do entrepreneurship and how students build companies. The Trust Center has recognized that for years, and we’ve welcomed AI into how we teach entrepreneurship at all levels, from the earliest stages of idea formation to exploring and testing those ideas and understanding how to commercialize and scale them.” AI’s strengths and weaknesses For the past few years, when the Trust Center’s delta v staff get together for strategic retreats, AI has been a central topic. The delta v program’s organizers think about how students can get the most out of the technology each year as they plan their summer-long curriculum. Everything starts with Orbit, the mobile app designed to help students find entrepreneurial resources, network with peers, access mentorship, and identify events and jobs. Jetpack was added to Orbit last year. It is trained on Aulet’s “Disciplined Entrepreneurship” as well as former Trust Center Executive Director Paul Cheek’s “ Startup Tactics ” book. The Trust Center describes Jetpack’s outputs as first drafts designed to help students brainstorm their next steps. “You need to verify everything when you are using AI to build a business,” says Kenney, who is also a lecturer at MIT Sloan and MIT D-Lab. “I have yet to meet anyone who will base their business on the output of something like ChatGPT without verifying everything first. Sometimes, the verification can take longer than if you had done the research yourself from the beginning.” One company in this year’s cohort, Mendhai Health, uses AI and telehealth to offer personalized physical therapy for women struggling with pelvic floor dysfunction before and after childbirth. “AI has definitely made the entrepreneurial process more efficient and faster,” says MBA student Aanchal Arora. “Still, overreliance on AI, at least at this point, can hamper your understanding of customers. You need to be careful with every decision you make.” Kenney notes the way large language models are built can make them less useful for entrepreneurs. “Some AI tools can increase your speed by doing things like automatically sorting your email or helping you vibe code apps, but many AI tools are built off averages, and those can be less effective when you’re trying to connect with a very specific demographic,” Kenney says. “It’s not helpful to have AI tell you about an average person, you need to personally have strong validation that your specific customer exists. If you try to build a tool for an average person, you may build a tool for no one at all.” Students eager to embrace AI may also be overwhelmed by the sheer volume of tools available today. Fortunately, MIT students have a long history of being at the forefront of any new technology, and this year’s delta v cohort featured teams leveraging AI at the core of their solutions and in every step of their entrepreneurial journeys. MIT Sloan MBA candidate Murtaza Jameel, whose company Cognify uses AI to simulates user interactions with websites and apps to improve digital experiences, describes his firm as an AI-native business. “We’re building a design intelligence tool that replaces product testing with instant, predictive simulations of user behavior,” Jameel explains. “We’re trying to integrate AI into all of our processes: ideation, go to market, programming. All of our building has been done with AI coding tools. I have a custom bot that I’ve fed tons of information about our company to, and it’s a thought partner I’m speaking to every single day.” The more things change… One of the fundamentals the Trust Center doesn’t see changing is the need for students to get out of the lab or the classroom to talk to customers. “There are ways that AI can unlock new capabilities and make things move faster, but we haven’t turned our curriculum on its head because of AI,” Soltoff says. “In delta v, we stress first and foremost: What are you building and who are you building it for? AI alone can’t tell you who your customer is, what they want, and how you can better serve their needs. You need to go out into the world to make that happen.” Indeed, many of the biggest hurdles delta v teams faced this summer looked a lot like the hurdles entrepreneurs have always faced. “We were prepared at the Trust Center to see a big change and to adapt to that, but the companies are still building and encountering the same challenges of customer identification, beachhead market identification, team dynamics,” Kenney says. “Those are still the big meaty challenges they’ve always been working on.” Amid endless hype about AI agents and the future of work, many founders this summer still said the human side of delta v is what makes the program special. “I came to MIT with one goal: to start a technology company,” Jameel says. “The delta v program was on my radar when I was applying to MIT. The program gives you incredible access to resources — networks, mentorship, advisors. Some of the top folks in our industry are advising us now on how to build our company. It’s really unique. These are folks who have done what you’re doing 10 or 20 years ago, all just rooting for you. That’s why I came to MIT.” Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "What does the future hold for generative AI?",
      "description": "At the inaugural MIT Generative AI Impact Consortium Symposium, researchers and business leaders discussed potential advancements centered on this powerful technology.",
      "url": "https://news.mit.edu/2025/what-does-future-hold-generative-ai-0919",
      "source": "MIT News AI",
      "published_date": "2025-09-19T04:00:00",
      "author": "Adam Zewe | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image When OpenAI introduced ChatGPT to the world in 2022, it brought generative artificial intelligence into the mainstream and started a snowball effect that led to its rapid integration into industry, scientific research, health care, and the everyday lives of people who use the technology. What comes next for this powerful but imperfect tool? With that question in mind, hundreds of researchers, business leaders, educators, and students gathered at MIT’s Kresge Auditorium for the inaugural MIT Generative AI Impact Consortium (MGAIC) Symposium on Sept. 17 to share insights and discuss the potential future of generative AI. “This is a pivotal moment — generative AI is moving fast. It is our job to make sure that, as the technology keeps advancing, our collective wisdom keeps pace,” said MIT Provost Anantha Chandrakasan to kick off this first symposium of the MGAIC, a consortium of industry leaders and MIT researchers launched in February to harness the power of generative AI for the good of society. Underscoring the critical need for this collaborative effort, MIT President Sally Kornbluth said that the world is counting on faculty, researchers, and business leaders like those in MGAIC to tackle the technological and ethical challenges of generative AI as the technology advances. “Part of MIT’s responsibility is to keep these advances coming for the world. … How can we manage the magic [of generative AI] so that all of us can confidently rely on it for critical applications in the real world?” Kornbluth said. To keynote speaker Yann LeCun, chief AI scientist at Meta, the most exciting and significant advances in generative AI will most likely not come from continued improvements or expansions of large language models like Llama, GPT, and Claude. Through training, these enormous generative models learn patterns in huge datasets to produce new outputs. Instead, LuCun and others are working on the development of “world models” that learn the same way an infant does — by seeing and interacting with the world around them through sensory input. “A 4-year-old has seen as much data through vision as the largest LLM. … The world model is going to become the key component of future AI systems,” he said. A robot with this type of world model could learn to complete a new task on its own with no training. LeCun sees world models as the best approach for companies to make robots smart enough to be generally useful in the real world. But even if future generative AI systems do get smarter and more human-like through the incorporation of world models, LeCun doesn’t worry about robots escaping from human control. Scientists and engineers will need to design guardrails to keep future AI systems on track, but as a society, we have already been doing this for millennia by designing rules to align human behavior with the common good, he said. “We are going to have to design these guardrails, but by construction, the system will not be able to escape those guardrails,” LeCun said. Keynote speaker Tye Brady, chief technologist at Amazon Robotics, also discussed how generative AI could impact the future of robotics. For instance, Amazon has already incorporated generative AI technology into many of its warehouses to optimize how robots travel and move material to streamline order processing. He expects many future innovations will focus on the use of generative AI in collaborative robotics by building machines that allow humans to become more efficient. “GenAI is probably the most impactful technology I have witnessed throughout my whole robotics career,” he said. Other presenters and panelists discussed the impacts of generative AI in businesses, from largescale enterprises like Coca-Cola and Analog Devices to startups like health care AI company Abridge. Several MIT faculty members also spoke about their latest research projects, including the use of AI to reduce noise in ecological image data, designing new AI systems that mitigate bias and hallucinations, and enabling LLMs to learn more about the visual world. After a day spent exploring new generative AI technology and discussing its implications for the future, MGAIC faculty co-lead Vivek Farias, the Patrick J. McGovern Professor at MIT Sloan School of Management, said he hoped attendees left with “a sense of possibility, and urgency to make that possibility real.” Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Samsung and SK join OpenAI’s Stargate initiative to advance global AI infrastructure",
      "description": "Samsung and SK join OpenAI’s Stargate initiative to expand global AI infrastructure, scaling advanced memory chip production and building next-gen data centers in Korea.",
      "url": "https://openai.com/index/samsung-and-sk-join-stargate",
      "source": "OpenAI Blog",
      "published_date": "2025-10-01T03:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "The Sora feed philosophy",
      "description": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "url": "https://openai.com/index/sora-feed-philosophy",
      "source": "OpenAI Blog",
      "published_date": "2025-09-30T10:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Launching Sora responsibly",
      "description": "To address the novel safety challenges posed by a state-of-the-art video model as well as a new social creation platform, we’ve built Sora 2 and the Sora app with safety at the foundation. Our approach is anchored in concrete protections.",
      "url": "https://openai.com/index/launching-sora-responsibly",
      "source": "OpenAI Blog",
      "published_date": "2025-09-30T00:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Sora 2 is here",
      "description": "Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.",
      "url": "https://openai.com/index/sora-2",
      "source": "OpenAI Blog",
      "published_date": "2025-09-30T00:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Sora 2 System Card",
      "description": "Sora 2 is our new state of the art video and audio generation model. Building on the foundation of Sora, this new model introduces capabilities that have been difficult for prior video models to achieve– such as more accurate physics, sharper realism, synchronized audio, enhanced steerability, and a...",
      "url": "https://openai.com/index/sora-2-system-card",
      "source": "OpenAI Blog",
      "published_date": "2025-09-30T00:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Building OpenAI with OpenAI",
      "description": "At OpenAI, we rely on our own technology to help streamline work, scale expertise, and drive outcomes. In our new series, OpenAI on OpenAI, we share lessons to help other organizations do the same.",
      "url": "https://openai.com/index/building-openai-with-openai",
      "source": "OpenAI Blog",
      "published_date": "2025-09-29T13:30:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Turning contracts into searchable data at OpenAI",
      "description": "OpenAI built a system to extract contract data quickly, cutting turnaround times and making it easier for teams to access the details they need.",
      "url": "https://openai.com/index/openai-contract-data-agent",
      "source": "OpenAI Blog",
      "published_date": "2025-09-29T13:30:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Converting inbound leads into customers at OpenAI",
      "description": "Learn how OpenAI used AI to deliver personalized answers at scale, converting inbound leads into customers.",
      "url": "https://openai.com/index/openai-inbound-sales-assistant",
      "source": "OpenAI Blog",
      "published_date": "2025-09-29T13:30:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    }
  ]
}