{
  "category": "AI/Tech",
  "scraped_at": "2025-10-07T11:54:21.124348",
  "total_articles": 37,
  "articles": [
    {
      "title": "IBM out With Agent Tools, AI Chip and Anthropic Alliance",
      "description": "AgentOps provides oversight and governance for AI agents, while features like Langflow Integration let users without technical skills build agents.",
      "url": "https://aibusiness.com/agentic-ai/ibm-out-with-agent-tools-ai-chip-anthropic-alliance",
      "source": "AI Business",
      "published_date": "2025-10-07T14:19:38",
      "author": "Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Survey Reveals AI Skills Shortage in U.K.",
      "description": "U.K. businesses commit to upskilling staff but struggle with execution, especially in AI and machine learning.",
      "url": "https://aibusiness.com/ml/survey-ai-skills-shortage-in-uk-",
      "source": "AI Business",
      "published_date": "2025-10-07T12:48:22",
      "author": "Graham Hope",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "RPA Stalwart UiPath Moves Into Agentic AI Realm",
      "description": "The transition to agentic AI was a logical step for the veteran robotic process automation vendor.",
      "url": "https://aibusiness.com/agentic-ai/uipath-moves-into-agentic-ai-realm",
      "source": "AI Business",
      "published_date": "2025-10-06T23:48:50",
      "author": "Shaun Sutner, Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Nvidia, Fujitsu Unveil AI, Robotics Partnership",
      "description": "The partners said their expanded collaboration will help support the 'AI industrial revolution.'",
      "url": "https://aibusiness.com/robotics/nvidia-fujitsu-ai-robotics-partnership",
      "source": "AI Business",
      "published_date": "2025-10-06T23:05:43",
      "author": "Scarlett Evans",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "OpenAI and AMD Strike Major AI Partnership",
      "description": "The multibillion-dollar deal between OpenAI with AMD for compute power could make AMD a key competitor to AI chip giant Nvidia.",
      "url": "https://aibusiness.com/generative-ai/openai-and-amd-strike-partnership",
      "source": "AI Business",
      "published_date": "2025-10-06T21:58:11",
      "author": "Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Report Shows How Consumer AI Tools Are Used by Enterprises",
      "description": "A new a16z report shows OpenAI tops AI spending by startups. Consumer AI tools are increasingly adopted by enterprises, with 70% not requiring enterprise licenses.",
      "url": "https://aibusiness.com/consumer-tech/report-consumer-ai-enterprise",
      "source": "AI Business",
      "published_date": "2025-10-03T20:05:01",
      "author": "Graham Hope",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "IEEE Publishes Framework for Humanoid Robot Standards",
      "description": "The new report comes as part of the IEEE’s wider project developing a roadmap for humanoid robot development.",
      "url": "https://aibusiness.com/robotics/ieee-framework-humanoid-robot-standards",
      "source": "AI Business",
      "published_date": "2025-10-03T15:19:19",
      "author": "Scarlett Evans",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Meta Launches Business AI Tools for SMBs",
      "description": "Business AI is accessible on Meta's platforms and on businesses' websites. Meta also unveiled augmented reality shopping experiences and customizable advertising options.",
      "url": "https://aibusiness.com/agentic-ai/meta-launches-business-ai-tools-smbs",
      "source": "AI Business",
      "published_date": "2025-10-02T20:07:53",
      "author": "Esther Shittu",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "How AI is changing the way we travel",
      "description": "AI is reshaping how people plan and experience travel. From curated videos on Instagram Reels to booking engines that build entire itineraries in seconds, AI is becoming a powerful force in how journeys are imagined, booked, and lived. But this shift raises an important question: is AI giving travel...",
      "url": "https://www.artificialintelligence-news.com/news/how-ai-is-changing-the-way-we-travel/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-07T11:00:00",
      "author": "Muhammad Zulhusni",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Google’s new AI agent rewrites code to automate vulnerability fixes",
      "description": "Google DeepMind has deployed a new AI agent designed to autonomously find and fix critical security vulnerabilities in software code. The system, aptly-named CodeMender, has already contributed 72 security fixes to established open-source projects in the last six months. Identifying and patching vul...",
      "url": "https://www.artificialintelligence-news.com/news/google-new-ai-agent-rewrites-code-automate-vulnerability-fixes/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-06T13:56:40",
      "author": "Ryan Daws",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "China Mobile Shanghai launches industry-first 5G-A network monetisation strategy with Huawei",
      "description": "The roar of 80,000 fans at Shanghai Stadium on September 21, 2025, wasn't just about the football match between Shanghai Shenhua and Chengdu Rongcheng – it was also a live demonstration of how telecom carriers are tackling one of their most pressing challenges: converting advanced network capabiliti...",
      "url": "https://www.artificialintelligence-news.com/news/5g-a-shanghai-huawei-network-monetization-football/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-03T09:00:00",
      "author": "Dashveenjit Kaur",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "AI causes reduction in users’ brain activity – MIT",
      "description": "A study from MIT (Massachusetts Institute of Technology) has found that the human brain not only works less hard when using an LLM, but its effects continue, negatively affecting mental activity in future work. The researchers used a limited number of subjects for their experiments (a limitation sta...",
      "url": "https://www.artificialintelligence-news.com/news/ai-causes-reduction-in-users-brain-activity-mit/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-01T13:44:30",
      "author": "Joe Green",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "The 5 best AI AppSec tools in 2025",
      "description": "Guest author: Or Hillel, Green Lamp Applications have become the foundation of how organisations deliver services, connect with customers, and manage important operations. Every transaction, interaction, and workflow runs on a web app, mobile interface, or API. That central role has made application...",
      "url": "https://www.artificialintelligence-news.com/news/the-5-best-ai-appsec-tools-in-2025/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-01T12:09:36",
      "author": "Or Hillel",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Why AI phishing detection will define cybersecurity in 2026",
      "description": "Reuters recently published a joint experiment with Harvard, where they asked popular AI chatbots like Grok, ChatGPT, DeepSeek, and others to craft the &#8220;perfect phishing email.&#8221; The generated emails were then sent to 108 volunteers, of whom 11% clicked on the malicious links. With one sim...",
      "url": "https://www.artificialintelligence-news.com/news/why-ai-phishing-detection-will-define-cybersecurity-in-2026/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-01T10:07:59",
      "author": "TechForge",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Google: EU’s AI adoption lags China amid regulatory hurdles",
      "description": "Google’s President of Global Affairs, Kent Walker, has urged the EU to increase AI adoption through a smarter regulatory approach amid increasing competition, particularly from China. Speaking at the Competitive Europe Summit in Brussels, Walker positioned AI as a tool that philosophers and economis...",
      "url": "https://www.artificialintelligence-news.com/news/google-eu-ai-adoption-lags-china-amid-regulatory-hurdles/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-10-01T09:54:47",
      "author": "Ryan Daws",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "The value gap from AI investments is widening dangerously fast",
      "description": "Boston Consulting Group (BCG) has found a widening chasm separating an elite of AI masters from the majority of firms struggling to generate any value from their AI investments. A study from BCG found that a mere five percent of companies are successfully achieving bottom-line value from AI at scale...",
      "url": "https://www.artificialintelligence-news.com/news/value-gap-ai-investments-widening-dangerously-fast/",
      "source": "Artificial Intelligence News",
      "published_date": "2025-09-30T12:35:19",
      "author": "Ryan Daws",
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "IBM Introduces the Spyre Accelerator for Commercial Availability",
      "description": "Coming this Fall to IBM Z, LinuxONE and Power, IBM Spyre Accelerator Enables Enterprises to Scale Generative and Agentic AI Workloads",
      "url": "https://newsroom.ibm.com/2025-10-07-ibm-introduces-the-spyre-accelerator-for-commercial-availability",
      "source": "IBM Think",
      "published_date": "2025-10-07T13:00:00",
      "author": null,
      "content": "ARMONK, N.Y. , Oct. 7, 2025 / PRNewswire / -- IBM (NYSE: IBM ) today announced the upcoming general availability of the IBM Spyre Accelerator, an AI accelerator enabling low-latency inferencing to support generative and agentic AI use cases while prioritizing the security and resilience of core workloads. Earlier this year, IBM announced the Spyre Accelerator would be available in IBM z17, LinuxONE 5, and Power11 systems. Spyre will be generally available on October 28 for IBM z17 and LinuxONE 5 systems, and in early December for Power11 servers. Today's IT landscape is changing from traditional logic workflows to agentic AI inferencing. AI agents require low-latency inference and real-time system responsiveness. IBM recognized the need for mainframes and servers to run AI models along with the most demanding enterprise workloads without compromising on throughput. To address this demand, clients need AI inferencing hardware that supports generative and agentic AI while maintaining the security and resilience of core data, transactions, and applications. The accelerator is also built to enable clients to keep mission-critical data on-prem to mitigate risk while addressing operational and energy efficiency. The IBM Spyre Accelerator reflects the strength of IBM's research-to-product pipeline, combining breakthrough innovation from the IBM Research AI Hardware Center with enterprise-grade development from IBM Infrastructure. Initially introduced as a prototype chip, Spyre was refined through rapid iteration, including cluster deployments at IBM's Yorktown Heights campus, and with collaborators like the University at Albany's Center for Emerging Artificial Intelligence Systems. The IBM Research prototype has evolved into an enterprise-grade product for use in IBM Z, LinuxONE and Power systems. Today, the Spyre Accelerator is a commercial system-on-a-chip with 32 individual accelerator cores and 25.6 billion transistors. Produced using 5nm node technology, each Spyre is mounted on a 75-watt PCIe card, which makes it possible to cluster up to 48 cards in an IBM Z or LinuxONE system or 16 cards in an IBM Power system to scale AI capabilities. \"One of our key priorities has been advancing infrastructure to meet the demands of new and emerging AI workloads,\" said Barry Baker , COO, IBM Infrastructure & GM, IBM Systems . \"With the Spyre Accelerator, we're extending the capabilities of our systems to support multi-model AI – including generative and agentic AI. This innovation positions clients to scale their AI-enabled mission-critical workloads with uncompromising security, resilience, and efficiency, while unlocking the value of their enterprise data.\" \"We launched the IBM Research AI Hardware Center in 2019 with a mission to meet the rising computational demands of AI, even before the surge in LLMs and AI models we've recently seen,\" said Mukesh Khare , GM of IBM Semiconductors and VP of Hybrid Cloud, IBM . \"Now, amid increasing demand for advanced AI capabilities, we're proud to see the first chip from the Center enter commercialization, designed to deliver improved performance and productivity to IBM's mainframe and server clients.\" For IBM clients, Spyre Accelerators offer fast, secured processing with on-prem AI acceleration. This marks a significant milestone, allowing businesses to leverage AI at scale while keeping data on IBM Z, LinuxONE and Power systems. In mainframe systems, coupled with the Telum II processor for IBM Z and LinuxONE, Spyre offers enhanced security, low latency, and high transaction rate processing power. Leveraging this advanced hardware and software stack, businesses can use Spyre to scale multiple AI models to power predictive use cases such as advanced fraud detection and retail automation. On IBM Power-based servers, Spyre customers can leverage a catalog of AI services, enabling end-to-end AI for enterprise workflows. Clients can install the AI services from the catalog with just one click. 1 Spyre Accelerator for Power, combined with an on-chip accelerator (MMA), also accelerates data conversion for generative AI to deliver high throughput for deep process integrations. Additionally, with a prompt size of 128, it enables the ingestion of more than 8 million documents for knowledge base integration in an hour 2 . This performance, combined with the IBM software stack, security, scalability, and energy efficiency, supports clients on their journey to integrating generative AI frameworks into their enterprise workloads. To learn more about the IBM Spyre Accelerator, visit http://www.ibm.com/solutions/ai-accelerator . About IBM IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain a competitive edge in their industries. Thousands of governments and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. Visit www.ibm.com for more information. Media Contacts Willa Hahn , willa.hahn@ibm.com Chase Skinner , Chase.Skinner@ibm.com 1 AI service of the IBM-supported catalog is delivered as one or a set of containers that can be deployed with a single deployment command. The provided UI for the catalog executes such commands in the backend based on a single click within the UI page of the respective AI service. 2 Based upon internal testing running 1M unit data set with prompt size 128, batch size 128 using 1-card container. Individual results may vary based on workload size, use of storage subsystems and other conditions.",
      "category": "AI/Tech"
    },
    {
      "title": "IBM Unveils Advancements Across Software and Infrastructure to Help Enterprises Operationalize AI",
      "description": "From Agentic Orchestration to Infrastructure Automation, New and Upcoming Capabilities Support Productivity for Developers, Lines of Business and Infrastructure",
      "url": "https://newsroom.ibm.com/2025-10-07-ibm-unveils-new-software-product-and-intelligent-infrastructure-capabilities-to-help-enterprises-operationalize-ai",
      "source": "IBM Think",
      "published_date": "2025-10-07T10:01:00",
      "author": null,
      "content": "Orlando, FL – October 7, 2025 – Today, at TechXchange 2025 , IBM’s (NYSE: IBM ) annual event for developers and technologists, the company unveiled new and upcoming product capabilities designed to help enterprises move beyond AI experimentation and unlock productivity gains across development, operations and business workflows. Drawing thousands of attendees from around the world, TechXchange serves as a launchpad for IBM’s latest advancements in agentic AI, hybrid cloud, quantum computing and intelligent infrastructure. Generative AI has the potential to add trillions in economic value in the coming years. Yet, many organizations face barriers to adoption – ranging from fragmented hybrid environments to gaps in data quality and AI readiness. IBM’s latest announcements address these challenges with products built for production readiness, real-time governance and seamless integration across hybrid cloud ecosystems. “AI productivity is the new speed of business. These features will help clients remove bottlenecks across their entire technology lifecycle,” said Dinesh Nirmal, Senior Vice President of Products, IBM Software . “With these enhancements across our portfolio, we’re giving customers capabilities that take developer productivity, agentic orchestration and infrastructure intelligence to the next level.” Enhancing Performance for Agentic Orchestration Core to IBM’s agentic AI framework is watsonx Orchestrate , a product offering more than 500 tools and customizable, domain-specific agents from IBM and its partners. Orchestrate is designed to be tool-agnostic and adaptable to any environment, enabling scalable deployment and governance of AI agents. Capabilities include AgentOps , a built-in agentic observability and governance layer that provides full lifecycle transparency. With real-time monitoring and policy-based controls, AgentOps help assess agents’ reliability. Consider an HR agent onboarding new employees: Businesses also require products that simplify agent setup and orchestration for both developers and non-technical teams. The following enhancements in watsonx Orchestrate aim to meet these needs: Additionally, IBM plans to extend these capabilities to the mainframe with the upcoming watsonx Assistant for Z . Purpose-built IBM Z agents will enable the shift from reactive troubleshooting to proactive system management by understanding conversational context and automating operational processes while maintaining security and compliance. Building on the IBM z17 launch, this redesigned experience aims to streamline workflows and deliver greater productivity to mainframe users. Advancing The Foundation That Gives Agents Trusted Context Following its acquisition of HashiCorp, IBM announced Project infragraph which replaces fragmented tools and manual processes with a unified, intelligent control plane for observability. As enterprises scale across hybrid and multi-cloud environments, tool sprawl creates information silos. For example, when a critical Common Vulnerabilities and Exposures (CVE) is discovered, the standard process is an individual will email dozens of teams across the organization and build a spreadsheet to manually track and ensure all known instances of the vulnerable component have been patched. Through Project infragraph, an organization can have one view of its entire infrastructure estate and security posture, without silos. It provides a live view — rather than manual CSV-based reporting — of what's being managed within HashiCorp Cloud Platform (HCP) and outside of it. The platform also allows users to drill down into any resource clusters in its infrastructure estate (such as any container running in a VPC) to see all components with near real-time data. Project infragraph is planned to be delivered as a capability within HCP. In the future, Project infragraph plans to extend HCP to connect to IBM’s broader software portfolio, including Red Hat Ansible and OpenShift, watsonx Orchestrate, Concert, Turbonomic, and Cloudability. This approach aims to help customers unify infrastructure, security, and applications under a consistent data and policy model. HashiCorp is now accepting applications for the private beta program for Project infragraph, which is expected to open in December 2025. Powering Developer Productivity Project Bob – now in private tech preview – is a new, AI-first integrated development environment (IDE) designed with advanced task generation capabilities for enterprise software development lifecycles (SDLC), including software modernization. Moving beyond today’s AI coding assistants, Project Bob seeks to fundamentally transform SDLC by working alongside developers to write, test, upgrade and help secure software. Project Bob uses and orchestrates between industry-leading LLMs including Anthropic Claude, Mistral AI, Llama, and IBM Granite. Key capabilities include: To request access to Project Bob, click here . Accelerating Enterprise AI Adoption With Choice A critical barrier preventing businesses from harnessing AI's full potential is the inability to access and deploy these technologies in ways that align with their unique enterprise requirements. Organizations need the flexibility to choose where, how, and with which AI technologies they operate without being locked into a single vendor's platform. IBM's expanding AI partner ecosystem aims to alleviate this challenge by delivering advanced AI tools, products, and services across IBM and other partner portfolios that can integrate seamlessly into a business’s existing environment and workflows. To that end, IBM is announcing a new partnership with Anthropic that continues this commitment to choice and flexibility. IBM will integrate Anthropic's large language models (LLMs) directly into select IBM software products, starting with Project Bob , its new AI-first IDE. IBM has also created a first-of-its-kind guide verified by Anthropic – Architecting Secure Enterprise AI Agents with MCP – focused on the Agent Development Lifecycle (ADLC), a structured approach to designing, deploying and managing enterprise AI agents. IBM TechXchange 2025 takes place October 6-9 in Orlando, FL. Stay up to speed on this news and more by visiting the IBM Newsroom . To read IBM recent client & partner testimonials, click here . Click here to access a suite of assets to share across your business and personal channels with our social sharing toolkit. Statements regarding IBM's future direction and intent are subject to change or withdrawal without notice, and represent goals and objectives only. IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain the competitive edge in their industries. Thousands of governments and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. Visit www.ibm.com for more information. Sarah Benchaita IBM Software Communications sarah.benchaita@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "IBM and Anthropic Partner to Advance Enterprise Software Development with Proven Security and Governance",
      "description": "Partnership integrates Claude into select internal and external development tools and enterprise products, aiming to deliver new productivity gains for IBM clients",
      "url": "https://newsroom.ibm.com/2025-10-07-2025-ibm-and-anthropic-partner-to-advance-enterprise-software-development-with-proven-security-and-governance",
      "source": "IBM Think",
      "published_date": "2025-10-07T10:00:00",
      "author": null,
      "content": "ARMONK, N.Y., October 7, 2025 – IBM (NYSE: IBM) and Anthropic today announced a strategic partnership to accelerate the development of enterprise-ready AI by infusing Anthropic’s Claude, one of the world’s most powerful family of large language models (LLMs), into IBM’s software portfolio to deliver measurable productivity gains, while building security, governance, and cost controls directly into the lifecycle of software development. Through the partnership, Claude will be integrated into select IBM software products, starting with IBM’s new AI-first integrated development environment (IDE), designed with advanced task generation capabilities for enterprise software development lifecycles (SDLC), including software modernization. The IDE is available in private preview to select IBM clients and in early testing, more than 6,000 early adopters within IBM are using the new IDE, reporting productivity gains averaging 45 percent, translating to meaningful cost savings while maintaining code quality and security standards. As organizations move from AI experimentation to production deployment, they need solutions that integrate seamlessly with existing enterprise infrastructure and meet strict IT requirements. IBM brings proven capabilities in enterprise software delivery, hybrid cloud architecture, and regulated industry expertise to ensure AI tools work within the complex realities of global business operations. “IBM has been the backbone of enterprise technology for decades because we understand what it takes to deploy at scale in mission-critical environments,” said Dinesh Nirmal, SVP, Software at IBM . “This partnership enhances our software portfolio with advanced AI capabilities while maintaining the governance, security, and reliability that our clients have come to expect. We’re giving development teams AI that fits how enterprises work not experimental tools that create new risks.” “Enterprises are looking for AI they can actually trust with their code, their data, and their day-to-day operations,” said Mike Krieger, Chief Product Officer at Anthropic . “Claude has become the go-to AI for developers at the world's largest companies because of our focus on safety and reliability. This partnership with IBM lets us bring that same level of dedication to even more enterprise teams while building the open standards that will make AI agents genuinely useful in business environments.” Automating the Software Development Lifecycle Using industry leading LLMs such as Claude, the new IBM IDE is built to enable developers to be more productive. It works in multiple languages and modes, but examples of potential leading use cases to help developers execute tasks across aspects of the software development lifecycle A Guide for Enterprise AI Agents with MCP Servers As part of the partnership, IBM has created, and Anthropic has verified Architecting Secure Enterprise AI Agents with MCP . A first-of-its-kind guide focusing on the Agent Development Lifecycle (ADLC), a structured approach to designing, deploying and managing enterprise AI agents. As organizations standardize on agentic AI to drive autonomous decision-making and intelligent automation, traditional IT processes will likely no longer be sufficient. This new ADLC methodology provides businesses with a purpose-built, enterprise-ready approach for the development, operations and security requirements for Enterprise AI agents . Beyond enhancing IBM’s software portfolio, IBM is contributing its enterprise technology leadership to advance open standards for AI deployment. IBM will contribute enterprise-grade assets to the Model Context Protocol (MCP) community, including best practices guides, reference architectures, and open-source tooling developed from IBM’s experience deploying AI across thousands of client environments. IBM is exploring plans to include Claude into additional IBM products as part of a product integration approach. Together, IBM and Anthropic are shaping the future of enterprise AI, one that empowers developers, drives transformation, and delivers long-term value for clients and society. Statements regarding IBM's and Anthropic’s future direction and intent are subject to change or withdrawal without notice, and represent goals and objectives only. IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs, and gain a competitive edge in their industries. Thousands of governments and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently, and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity, and service. Visit www.ibm.com for more information. Elizabeth Brophy IBM Elizabeth.Brophy@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "IBM and AMD Collaborate with Zyphra on Next Generation AI Infrastructure",
      "description": "Open-source superintelligence company leverages new integrated capabilities for AMD training clusters on IBM Cloud",
      "url": "https://newsroom.ibm.com/2025-10-01-ibm-and-amd-collaborate-with-zyphra-on-next-generation-ai-infrastructure",
      "source": "IBM Think",
      "published_date": "2025-10-01T11:04:00",
      "author": null,
      "content": "ARMONK, N.Y. and AUSTIN, Texas , Oct. 1, 2025 / PRNewswire / -- IBM (NYSE: IBM ) and AMD (NASDAQ: AMD) today announced a collaboration to deliver advanced AI infrastructure to Zyphra, an open-source AI research and product company based in San Francisco, California . Under a multi-year agreement between IBM and Zyphra, IBM is positioned to deliver a large cluster of AMD Instinct™ MI300X GPUs on IBM Cloud for Zyphra to use for training frontier multimodal foundation models. This collaboration is expected to deliver among the largest advanced generative AI training capabilities to date powered by an AMD stack running on IBM Cloud. Zyphra recently closed a Series A financing round at a $1B valuation to build a leading open-source/open-science superintelligence lab focused on advancing fundamental innovations in novel neural network architectures, long-term memory, and continual learning. Zyphra partnered with IBM and AMD for their cutting-edge product roadmaps and ability to deliver GPU accelerators at the rapid pace required to drive Zyphra's innovation forward. This agreement is the first large-scale, dedicated training cluster on IBM Cloud leveraging AMD Instinct MI300X GPUs and AMD Pensando™ Pollara 400 AI NICs and AMD Pensando Ortano DPUs. The initial deployment was made available to Zyphra in early September with planned expansion in 2026. Photo credit: AMD Zyphra will use the advanced training cluster to develop multimodal foundation models across language, vision and audio modalities to power Maia, a general purpose superagent designed to deliver productivity benefits for knowledge workers across enterprise. IBM and AMD are uniquely positioned to continue scaling computational resources as Zyphra's AI model training needs expand. \"This collaboration marks the first time AMD's full-stack training platform—spanning compute through networking—has been successfully integrated and scaled on IBM Cloud, and Zyphra is honored to lead the way in developing frontier models with AMD silicon on IBM Cloud,\" said Krithik Puthalath, CEO and Chairman of Zyphra . \"We're excited to partner with IBM and AMD to power the next era of open-source, enterprise superintelligence.\" IBM and AMD Unlocking a New Era in AI Training Infrastructure Last year, IBM and AMD announced a collaboration to deploy AMD Instinct MI300X accelerators as a service on IBM Cloud. Known for its security, reliability and scalability, IBM Cloud's robust infrastructure complements the capabilities of the AMD Instinct MI300X. This offering was designed to enhance performance and power efficiency for Gen AI models and high-performance computing (HPC) applications. \"Scaling AI workloads faster and more efficiently is a key differentiator in achieving ROI for established enterprises and emerging companies alike,\" said Alan Peacock , GM of IBM Cloud . \"We are delighted to support Zyphra's strategic roadmap as we collaborate with AMD to deliver scalable, economical AI infrastructure that can accelerate Zyphra's model training.\" \"The IBM and AMD collaboration delivers innovation at the speed and scale clients demand, representing a new standard in AI infrastructure,\" said Philip Guido , EVP and Chief Commercial Officer, AMD . \"By combining IBM enterprise cloud expertise with AMD leadership in high-performance computing and AI acceleration, we are supporting Zyphra's pioneering work in multimodal and inference-efficient AI, enabling organizations everywhere to build smarter businesses and unlock AI solutions that drive real-world outcomes.\" IBM and AMD are forging new ground in AI infrastructure, helping redefine performance, efficiency, and scale for enterprise and startup customers. Their hybrid infrastructure environment offers a foundation for scaling AI, with options such as hybrid multi-cloud that can help boost cloud ROI and value for clients' generative AI deployments. IBM and AMD also recently announced plans to develop next-generation computing architectures, known as quantum-centric supercomputing, leveraging IBM's leadership in developing the world's most performant quantum computers and software, and AMD's leadership in high-performance computing and AI accelerators. For more information about the collaboration between IBM Cloud and AMD, please visit: https://www.ibm.com/products/gpu-ai-accelerator/amd IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain a competitive edge in their industries. Thousands of governments and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's long-standing commitment to trust, transparency, responsibility, inclusivity and service. Visit www.ibm.com for more information. For more than 55 years AMD has driven innovation in high-performance computing, graphics and visualization technologies. Billions of people, leading Fortune 500 businesses and cutting edge scientific research institutions around the world rely on AMD technology daily to improve how they live, work and play. AMD employees are focused on building leadership high- 3 performance and adaptive products that push the boundaries of what is possible. For more information about how AMD is enabling today and inspiring tomorrow, visit the AMD (NASDAQ: AMD) website , blog , LinkedIn , Facebook and X pages. Media Contact Kate Gazzillo kate.gazzillo@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "HashiCorp Previews the Future of Agentic Infrastructure Automation with Project infragraph",
      "description": "New Infrastructure and Security Lifecycle capabilities simplify hybrid operations and help move toward intelligent infrastructure operations",
      "url": "https://newsroom.ibm.com/2025-09-25-hashicorp-previews-the-future-of-agentic-infrastructure-automation-with-project-infragraph",
      "source": "IBM Think",
      "published_date": "2025-09-25T16:01:00",
      "author": null,
      "content": "SAN FRANCISCO , Sept. 25, 2025 / PRNewswire / -- Today at HashiConf 2025, the 10th global conference hosted by HashiCorp and its first as an IBM (NYSE: IBM ) company, the company introduced a series of Infrastructure and Security Lifecycle innovations, and a preview of Project infragraph — a new strategic investment for the HashiCorp Cloud Platform (HCP) that lays the groundwork for agentic infrastructure. Infrastructure as code and identity-based security are typically foundational practices for cloud programs. But complexity continues to grow as organizations work to operationalize AI, and infrastructure can require more intelligence, integration, and autonomous operations. These announcements reflect this shift, built to advance the capabilities needed to operate efficiently today, while helping teams prepare for agentic workflows. Project infragraph from HashiCorp, an IBM company, is a real-time infrastructure graph that connects infrastructure, applications, services, and ownership. Introducing Project infragraph: The foundation for agentic infrastructure As part of IBM, HashiCorp is accelerating its vision to deliver a unified control plane that extends across the hybrid cloud to support organizations of all sizes, operating across cloud environments. Modern enterprises lack a unified system of record for infrastructure and security. Visibility can be fragmented, context lost, and Day 2 operations suffer. Project infragraph looks to solve these challenges, as a real-time infrastructure graph that connects infrastructure, applications, services, and ownership. Project infragraph is planned to be delivered as a capability within the HashiCorp Cloud Platform (HCP). In the future, Project infragraph plans to extend HCP to connect to IBM's broader software portfolio, including Red Hat Ansible and OpenShift, IBM watsonx Orchestrate, Concert, Turbonomic, and Cloudability. This approach will help customers unify infrastructure, security, and applications under a consistent data and policy model. With Project infragraph, infrastructure teams can begin solving long-standing challenges around visibility, ownership, and data governance—without the complexity of fragmented tooling. The vision of Project infragraph is that over time, as more capabilities are added, the same graph will enable AI to reason about infrastructure state, propose runbooks and configuration changes, and effectively act across the application lifecycle. HashiCorp is now accepting applications for the private beta program for Project infragraph, which is expected to open in December 2025 . From Day 0 to Day N: What's new in ILM and SLM Key Infrastructure Lifecycle Management (ILM) and Security Lifecycle Management (SLM) updates demonstrate how HashiCorp is helping teams address today's infrastructure and security challenges—while advancing towards intelligent operations. Infrastructure Lifecycle Management (ILM) New ILM capabilities focus on making infrastructure provisioning, policy governance, and Day 2 operations faster, and more scalable across complex, hybrid environments. Security Lifecycle Management (SLM) New SLM enhancements improve secrets detection, simplify secure access, and support policy governance for modern enterprise environments. \"HashiCorp's latest product updates and the introduction of Project infragraph signal more than product momentum—they represent the evolution of a platform that can unify infrastructure and security data, and accelerate intelligent decision-making,\" said Armon Dadgar , CTO and co-founder of HashiCorp. \"We're focused on helping customers build secured, scalable cloud programs that are ready for AI and drive value to every stakeholder.\" \"Project infragraph is a major step toward infrastructure that can observe, reason, and act,\" said Dinesh Nirmal , Senior Vice President, IBM Software. \"By combining automation with real-time infrastructure intelligence, we are creating the control layer that unlocks the next era of AI-powered operations.\" Information about HashiConf 2025 HashiConf is HashiCorp's global cloud conference, featuring 2+ days of conversations on the future of cloud automation with product announcements, technical sessions, hands-on labs, certifications, social events, and more. HashiConf 2025 is sponsored by AWS, Microsoft, Arrow, Atyeti, Coder, Clumio, Datadog, Gomboc, Google Cloud, Mondoo, Overmind, Palo Alto Networks, Red Hat, River Point Technology, TD Synnex, and Wiz. To register for a free virtual pass to HashiConf — with access to a dedicated platform to view the live-streamed keynotes, educational content, and live chat with online attendees, as well as access to all virtual sessions on demand after the event — visit the conference website . All product announcements are available as referenced above, with more details available at hashicorp.com . Organizations interested in shaping the future of agentic infrastructure automation are invited to apply for the Project infragraph private beta . IBM's statements regarding future directions and intentions are subject to change or withdrawal without notice and represent goals and objectives only. HashiCorp, an IBM company, helps organizations automate hybrid cloud environments with Infrastructure and Security Lifecycle Management. HashiCorp offers The Infrastructure Cloud on the HashiCorp Cloud Platform (HCP) for managed cloud services, as well as self-hosted enterprise offerings and community source-available products. For more information, visit hashicorp.com. All product and company names are trademarks or registered trademarks of their respective holders. Red Hat, the Red Hat logo, OpenShift and Ansible are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries. Media & Analyst Contact: IBM Matt Marcus matt.marcus@ibm.com",
      "category": "AI/Tech"
    },
    {
      "title": "New prediction model could improve the reliability of fusion power plants",
      "description": "The approach combines physics and machine learning to avoid damaging disruptions when powering down tokamak fusion machines.",
      "url": "https://news.mit.edu/2025/new-prediction-model-could-improve-reliability-fusion-power-plants-1007",
      "source": "MIT News AI",
      "published_date": "2025-10-07T04:00:00",
      "author": "Jennifer Chu | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image Tokamaks are machines that are meant to hold and harness the power of the sun. These fusion machines use powerful magnets to contain a plasma hotter than the sun’s core and push the plasma’s atoms to fuse and release energy. If tokamaks can operate safely and efficiently, the machines could one day provide clean and limitless fusion energy. Today, there are a number of experimental tokamaks in operation around the world, with more underway. Most are small-scale research machines built to investigate how the devices can spin up plasma and harness its energy. One of the challenges that tokamaks face is how to safely and reliably turn off a plasma current that is circulating at speeds of up to 100 kilometers per second, at temperatures of over 100 million degrees Celsius. Such “rampdowns” are necessary when a plasma becomes unstable. To prevent the plasma from further disrupting and potentially damaging the device’s interior, operators ramp down the plasma current. But occasionally the rampdown itself can destabilize the plasma. In some machines, rampdowns have caused scrapes and scarring to the tokamak’s interior — minor damage that still requires considerable time and resources to repair. Now, scientists at MIT have developed a method to predict how plasma in a tokamak will behave during a rampdown. The team combined machine-learning tools with a physics-based model of plasma dynamics to simulate a plasma’s behavior and any instabilities that may arise as the plasma is ramped down and turned off. The researchers trained and tested the new model on plasma data from an experimental tokamak in Switzerland. They found the method quickly learned how plasma would evolve as it was tuned down in different ways. What’s more, the method achieved a high level of accuracy using a relatively small amount of data. This training efficiency is promising, given that each experimental run of a tokamak is expensive and quality data is limited as a result. The new model, which the team highlights this week in an open-access Nature Communications paper , could improve the safety and reliability of future fusion power plants. “For fusion to be a useful energy source it’s going to have to be reliable,” says lead author Allen Wang, a graduate student in aeronautics and astronautics and a member of the Disruption Group at MIT’s Plasma Science and Fusion Center (PSFC). “To be reliable, we need to get good at managing our plasmas.” The study’s MIT co-authors include PSFC Principal Research Scientist and Disruptions Group leader Cristina Rea, and members of the Laboratory for Information and Decision Systems (LIDS) Oswin So, Charles Dawson, and Professor Chuchu Fan, along with Mark (Dan) Boyer of Commonwealth Fusion Systems and collaborators from the Swiss Plasma Center in Switzerland. “A delicate balance” Tokamaks are experimental fusion devices that were first built in the Soviet Union in the 1950s. The device gets its name from a Russian acronym that translates to a “toroidal chamber with magnetic coils.” Just as its name describes, a tokamak is toroidal, or donut-shaped, and uses powerful magnets to contain and spin up a gas to temperatures and energies high enough that atoms in the resulting plasma can fuse and release energy. Today, tokamak experiments are relatively low-energy in scale, with few approaching the size and output needed to generate safe, reliable, usable energy. Disruptions in experimental, low-energy tokamaks are generally not an issue. But as fusion machines scale up to grid-scale dimensions, controlling much higher-energy plasmas at all phases will be paramount to maintaining a machine’s safe and efficient operation. “Uncontrolled plasma terminations, even during rampdown, can generate intense heat fluxes damaging the internal walls,” Wang notes. “Quite often, especially with the high-performance plasmas, rampdowns actually can push the plasma closer to some instability limits. So, it’s a delicate balance. And there’s a lot of focus now on how to manage instabilities so that we can routinely and reliably take these plasmas and safely power them down. And there are relatively few studies done on how to do that well.” Bringing down the pulse Wang and his colleagues developed a model to predict how a plasma will behave during tokamak rampdown. While they could have simply applied machine-learning tools such as a neural network to learn signs of instabilities in plasma data, “you would need an ungodly amount of data” for such tools to discern the very subtle and ephemeral changes in extremely high-temperature, high-energy plasmas, Wang says. Instead, the researchers paired a neural network with an existing model that simulates plasma dynamics according to the fundamental rules of physics. With this combination of machine learning and a physics-based plasma simulation, the team found that only a couple hundred pulses at low performance, and a small handful of pulses at high performance, were sufficient to train and validate the new model. The data they used for the new study came from the TCV, the Swiss “variable configuration tokamak” operated by the Swiss Plasma Center at EPFL (the Swiss Federal Institute of Technology Lausanne). The TCV is a small experimental fusion experimental device that is used for research purposes, often as test bed for next-generation device solutions. Wang used the data from several hundred TCV plasma pulses that included properties of the plasma such as its temperature and energies during each pulse’s ramp-up, run, and ramp-down. He trained the new model on this data, then tested it and found it was able to accurately predict the plasma’s evolution given the initial conditions of a particular tokamak run. The researchers also developed an algorithm to translate the model’s predictions into practical “trajectories,” or plasma-managing instructions that a tokamak controller can automatically carry out to for instance adjust the magnets or temperature maintain the plasma’s stability. They implemented the algorithm on several TCV runs and found that it produced trajectories that safely ramped down a plasma pulse, in some cases faster and without disruptions compared to runs without the new method. “At some point the plasma will always go away, but we call it a disruption when the plasma goes away at high energy. Here, we ramped the energy down to nothing,” Wang notes. “We did it a number of times. And we did things much better across the board. So, we had statistical confidence that we made things better.” The work was supported in part by Commonwealth Fusion Systems (CFS), an MIT spinout that intends to build the world’s first compact, grid-scale fusion power plant. The company is developing a demo tokamak, SPARC, designed to produce net-energy plasma, meaning that it should generate more energy than it takes to heat up the plasma. Wang and his colleagues are working with CFS on ways that the new prediction model and tools like it can better predict plasma behavior and prevent costly disruptions to enable safe and reliable fusion power. “We’re trying to tackle the science questions to make fusion routinely useful,” Wang says. “What we’ve done here is the start of what is still a long journey. But I think we’ve made some nice progress.” Additional support for the research came from the framework of the EUROfusion Consortium, via the Euratom Research and Training Program and funded by the Swiss State Secretariat for Education, Research, and Innovation. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Printable aluminum alloy sets strength records, may enable lighter aircraft parts",
      "description": "Incorporating machine learning, MIT engineers developed a way to 3D print alloys that are much stronger than conventionally manufactured versions.",
      "url": "https://news.mit.edu/2025/printable-aluminum-alloy-sets-strength-records-may-enable-lighter-aircraft-parts-1007",
      "source": "MIT News AI",
      "published_date": "2025-10-07T04:00:00",
      "author": "Jennifer Chu | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image MIT engineers have developed a printable aluminum alloy that can withstand high temperatures and is five times stronger than traditionally manufactured aluminum. The new printable metal is made from a mix of aluminum and other elements that the team identified using a combination of simulations and machine learning, which significantly pruned the number of possible combinations of materials to search through. While traditional methods would require simulating over 1 million possible combinations of materials, the team’s new machine learning-based approach needed only to evaluate 40 possible compositions before identifying an ideal mix for a high-strength, printable aluminum alloy. When they printed the alloy and tested the resulting material, the team confirmed that, as predicted, the aluminum alloy was as strong as the strongest aluminum alloys that are manufactured today using traditional casting methods. The researchers envision that the new printable aluminum could be made into stronger, more lightweight and temperature-resistant products, such as fan blades in jet engines. Fan blades are traditionally cast from titanium — a material that is more than 50 percent heavier and up to 10 times costlier than aluminum — or made from advanced composites. “If we can use lighter, high-strength material, this would save a considerable amount of energy for the transportation industry,” says Mohadeseh Taheri-Mousavi, who led the work as a postdoc at MIT and is now an assistant professor at Carnegie Mellon University. “Because 3D printing can produce complex geometries, save material, and enable unique designs, we see this printable alloy as something that could also be used in advanced vacuum pumps, high-end automobiles, and cooling devices for data centers,” adds John Hart, the Class of 1922 Professor and head of the Department of Mechanical Engineering at MIT. Hart and Taheri-Mousavi provide details on the new printable aluminum design in a paper published in the journal Advanced Materials . The paper’s MIT co-authors include Michael Xu, Clay Houser, Shaolou Wei, James LeBeau, and Greg Olson, along with Florian Hengsbach and Mirko Schaper of Paderborn University in Germany, and Zhaoxuan Ge and Benjamin Glaser of Carnegie Mellon University. The new work grew out of an MIT class that Taheri-Mousavi took in 2020, which was taught by Greg Olson, professor of the practice in the Department of Materials Science and Engineering. As part of the class, students learned to use computational simulations to design high-performance alloys. Alloys are materials that are made from a mix of different elements, the combination of which imparts exceptional strength and other unique properties to the material as a whole. Olson challenged the class to design an aluminum alloy that would be stronger than the strongest printable aluminum alloy designed to date. As with most materials, the strength of aluminum depends in large part on its microstructure: The smaller and more densely packed its microscopic constituents, or “precipitates,” the stronger the alloy would be. With this in mind, the class used computer simulations to methodically combine aluminum with various types and concentrations of elements, to simulate and predict the resulting alloy’s strength. However, the exercise failed to produce a stronger result. At the end of the class, Taheri-Mousavi wondered: Could machine learning do better? “At some point, there are a lot of things that contribute nonlinearly to a material’s properties, and you are lost,” Taheri-Mousavi says. “With machine-learning tools, they can point you to where you need to focus, and tell you for example, these two elements are controlling this feature. It lets you explore the design space more efficiently.” Layer by layer In the new study, Taheri-Mousavi continued where Olson’s class left off, this time looking to identify a stronger recipe for aluminum alloy. This time, she used machine-learning techniques designed to efficiently comb through data such as the properties of elements, to identify key connections and correlations that should lead to a more desirable outcome or product. She found that, using just 40 compositions mixing aluminum with different elements, their machine-learning approach quickly homed in on a recipe for an aluminum alloy with higher volume fraction of small precipitates, and therefore higher strength, than what the previous studies identified. The alloy’s strength was even higher than what they could identify after simulating over 1 million possibilities without using machine learning. To physically produce this new strong, small-precipitate alloy, the team realized 3D printing would be the way to go instead of traditional metal casting, in which molten liquid aluminum is poured into a mold and is left to cool and harden. The longer this cooling time is, the more likely the individual precipitate is to grow. The researchers showed that 3D printing, broadly also known as additive manufacturing, can be a faster way to cool and solidify the aluminum alloy. Specifically, they considered laser bed powder fusion (LBPF) — a technique by which a powder is deposited, layer by layer, on a surface in a desired pattern and then quickly melted by a laser that traces over the pattern. The melted pattern is thin enough that it solidfies quickly before another layer is deposited and similarly “printed.” The team found that LBPF’s inherently rapid cooling and solidification enabled the small-precipitate, high-strength aluminum alloy that their machine learning method predicted. “Sometimes we have to think about how to get a material to be compatible with 3D printing,” says study co-author John Hart. “Here, 3D printing opens a new door because of the unique characteristics of the process — particularly, the fast cooling rate. Very rapid freezing of the alloy after it’s melted by the laser creates this special set of properties.” Putting their idea into practice, the researchers ordered a formulation of printable powder, based on their new aluminum alloy recipe. They sent the powder — a mix of aluminum and five other elements — to collaborators in Germany, who printed small samples of the alloy using their in-house LPBF system. The samples were then sent to MIT where the team ran multiple tests to measure the alloy’s strength and image the samples’ microstructure. Their results confirmed the predictions made by their initial machine learning search: The printed alloy was five times stronger than a casted counterpart and 50 percent stronger than alloys designed using conventional simulations without machine learning. The new alloy’s microstructure also consisted of a higher volume fraction of small precipitates, and was stable at high temperatures of up to 400 degrees Celsius — a very high temperature for aluminum alloys. The researchers are applying similar machine-learning techniques to further optimize other properties of the alloy. “Our methodology opens new doors for anyone who wants to do 3D printing alloy design,” Taheri-Mousavi says. “My dream is that one day, passengers looking out their airplane window will see fan blades of engines made from our aluminum alloys.” This work was carried out, in part, using MIT.nano’s characterization facilities. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "AI maps how a new antibiotic targets gut bacteria",
      "description": "MIT CSAIL and McMaster researchers used a generative AI model to reveal how a narrow-spectrum antibiotic attacks disease-causing bacteria, speeding up a process that normally takes years.",
      "url": "https://news.mit.edu/2025/ai-maps-how-new-antibiotic-targets-gut-bacteria-1003",
      "source": "MIT News AI",
      "published_date": "2025-10-03T21:00:00",
      "author": "Rachel Gordon | MIT CSAIL",
      "content": "Previous image Next image For patients with inflammatory bowel disease, antibiotics can be a double-edged sword. The broad-spectrum drugs often prescribed for gut flare-ups can kill helpful microbes alongside harmful ones, sometimes worsening symptoms over time. When fighting gut inflammation, you don’t always want to bring a sledgehammer to a knife fight. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and McMaster University have identified a new compound that takes a more targeted approach. The molecule, called enterololin, suppresses a group of bacteria linked to Crohn’s disease flare-ups while leaving the rest of the microbiome largely intact. Using a generative AI model, the team mapped how the compound works, a process that usually takes years but was accelerated here to just months. “This discovery speaks to a central challenge in antibiotic development,” says Jon Stokes, senior author of a new paper on the work , assistant professor of biochemistry and biomedical sciences at McMaster, and research affiliate at MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health. “The problem isn’t finding molecules that kill bacteria in a dish — we’ve been able to do that for a long time. A major hurdle is figuring out what those molecules actually do inside bacteria. Without that detailed understanding, you can’t develop these early-stage antibiotics into safe and effective therapies for patients.” Enterololin is a stride toward precision antibiotics: treatments designed to knock out only the bacteria causing trouble. In mouse models of Crohn’s-like inflammation, the drug zeroed in on Escherichia coli , a gut-dwelling bacterium that can worsen flares, while leaving most other microbial residents untouched. Mice given enterololin recovered faster and maintained a healthier microbiome than those treated with vancomycin, a common antibiotic. Pinning down a drug’s mechanism of action, the molecular target it binds inside bacterial cells, normally requires years of painstaking experiments. Stokes’ lab discovered enterololin using a high-throughput screening approach, but determining its target would have been the bottleneck. Here, the team turned to DiffDock , a generative AI model developed at CSAIL by MIT PhD student Gabriele Corso and MIT Professor Regina Barzilay. DiffDock was designed to predict how small molecules fit into the binding pockets of proteins, a notoriously difficult problem in structural biology. Traditional docking algorithms search through possible orientations using scoring rules, often producing noisy results. DiffDock instead frames docking as a probabilistic reasoning problem: a diffusion model iteratively refines guesses until it converges on the most likely binding mode. “In just a couple of minutes, the model predicted that enterololin binds to a protein complex called LolCDE, which is essential for transporting lipoproteins in certain bacteria,” says Barzilay, who also co-leads the Jameel Clinic. “That was a very concrete lead — one that could guide experiments, rather than replace them.” Stokes’ group then put that prediction to the test. Using DiffDock predictions as an experimental GPS, they first evolved enterololin-resistant mutants of E. coli in the lab, which revealed that changes in the mutant’s DNA mapped to lolCDE, precisely where DiffDock had predicted enterololin to bind. They also performed RNA sequencing to see which bacterial genes switched on or off when exposed to the drug, as well as used CRISPR to selectively knock down expression of the expected target. These laboratory experiments all revealed disruptions in pathways tied to lipoprotein transport, exactly what DiffDock had predicted. “When you see the computational model and the wet-lab data pointing to the same mechanism, that’s when you start to believe you’ve figured something out,” says Stokes. For Barzilay, the project highlights a shift in how AI is used in the life sciences. “A lot of AI use in drug discovery has been about searching chemical space, identifying new molecules that might be active,” she says. “What we’re showing here is that AI can also provide mechanistic explanations, which are critical for moving a molecule through the development pipeline.” That distinction matters because mechanism-of-action studies are often a major rate-limiting step in drug development. Traditional approaches can take 18 months to two years, or more, and cost millions of dollars. In this case, the MIT–McMaster team cut the timeline to about six months, at a fraction of the cost. Enterololin is still in the early stages of development, but translation is already underway. Stokes’ spinout company, Stoked Bio, has licensed the compound and is optimizing its properties for potential human use. Early work is also exploring derivatives of the molecule against other resistant pathogens, such as Klebsiella pneumoniae . If all goes well, clinical trials could begin within the next few years. The researchers also see broader implications. Narrow-spectrum antibiotics have long been sought as a way to treat infections without collateral damage to the microbiome, but they have been difficult to discover and validate. AI tools like DiffDock could make that process more practical, rapidly enabling a new generation of targeted antimicrobials. For patients with Crohn’s and other inflammatory bowel conditions, the prospect of a drug that reduces symptoms without destabilizing the microbiome could mean a meaningful improvement in quality of life. And in the bigger picture, precision antibiotics may help tackle the growing threat of antimicrobial resistance. “What excites me is not just this compound, but the idea that we can start thinking about the mechanism of action elucidation as something we can do more quickly, with the right combination of AI, human intuition, and laboratory experiments,” says Stokes. “That has the potential to change how we approach drug discovery for many diseases, not just Crohn’s.” “One of the greatest challenges to our health is the increase of antimicrobial-resistant bacteria that evade even our best antibiotics,” adds Yves Brun, professor at the University of Montreal and distinguished professor emeritus at Indiana University Bloomington, who wasn’t involved in the paper. “AI is becoming an important tool in our fight against these bacteria. This study uses a powerful and elegant combination of AI methods to determine the mechanism of action of a new antibiotic candidate, an important step in its potential development as a therapeutic.” Corso, Barzilay, and Stokes wrote the paper with McMaster researchers Denise B. Catacutan, Vian Tran, Jeremie Alexander, Yeganeh Yousefi, Megan Tu, Stewart McLellan, and Dominique Tertigas, and professors ​​Jakob Magolan, Michael Surette, Eric Brown, and Brian Coombes. Their research was supported, in part, by the Weston Family Foundation; the David Braley Centre for Antibiotic Discovery; the Canadian Institutes of Health Research; the Natural Sciences and Engineering Research Council of Canada; M. and M. Heersink; Canadian Institutes for Health Research; Ontario Graduate Scholarship Award; the Jameel Clinic; and the U.S. Defense Threat Reduction Agency Discovery of Medical Countermeasures Against New and Emerging Threats program. The researchers posted sequencing data in public repositories and released the DiffDock-L code openly on GitHub. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Martin Trust Center for MIT Entrepreneurship welcomes Ana Bakshi as new executive director",
      "description": "Bakshi will help shape and scale entrepreneurship education and platform at MIT.",
      "url": "https://news.mit.edu/2025/martin-trust-center-mit-entrepreneurship-ana-bakshi-executive-director-1002",
      "source": "MIT News AI",
      "published_date": "2025-10-02T19:55:00",
      "author": "Martin Trust Center for MIT Entrepreneurship",
      "content": "Previous image Next image The Martin Trust Center for MIT Entrepreneurship announced that Ana Bakshi has been named its new executive director. Bakshi stepped into the role at the start of the fall semester and will collaborate closely with the managing director, Ethernet Inventors Professor of the Practice Bill Aulet, to elevate the center to higher levels. “Ana is uniquely qualified for this role. She brings a deep and highly decorated background in entrepreneurship education at the highest levels, along with exceptional leadership and execution skills,” says Aulet. “Since I first met her 12 years ago, I have been extraordinarily impressed with her commitment to create the highest-quality centers and institutes for entrepreneurs, first at King’s College London and then at Oxford University. This ideal skill set is compounded by her experience in leading high-growth companies, most recently as the chief operation officer in an award-winning AI startup. I’m honored and thrilled to welcome her to MIT — her knowledge and energy will greatly elevate our community, and the field as a whole.” A rapidly changing environment creates imperative for raising the bar for entrepreneurship education The need to raise the bar for innovation-driven entrepreneurship education is both timely and urgent. The rate of change is getting faster and faster every day, especially with artificial intelligence, and is generating new problems that need to be solved, as well as exacerbating existing problems in climate, health care, manufacturing, future of work, education, and economic stratification, to name but a few. The world needs more entrepreneurs and better entrepreneurs. Bakshi joins the Trust Center at an exciting time in its history. MIT is at the forefront of helping to develop people and systems that can turn challenges into opportunities using an entrepreneurial mindset, skill set, and way of operating. Bakshi’s deep experience and success will be key to unlocking this opportunity. “I am truly honored to join the Trust Center at such a pivotal moment,” Bakshi says. “In an era defined by both extraordinary challenges and extraordinary possibilities, the future will be built by those bold enough to try, and MIT will be at the forefront of this.” Translating academic research into real-world impact Bakshi has a decade of experience building two world-class entrepreneurship centers from the ground up. She served as the founding director at King’s College and then at Oxford. In this role, she was responsible for all aspects of these centers, including fundraising. While at Oxford, she authored a data-driven approach to determining efficacy of outcomes for their programs, as evidenced by a 61-page study, “Universities: Drivers of Prosperity and Economic Recovery.” As the director of the Oxford Foundry (Oxford’s cross-university entrepreneurship center), Bakshi focused on investing in ambitious founders and talent. The center was backed by global entrepreneurial leaders such as the founders of LinkedIn and Twitter, with corporate partnerships including Santander and EY, and investment funds including Oxford Science Enterprises (OSE). As of 2021, the startups supported by the Foundry and King’s College have raised over $500 million and have created nearly 3,000 jobs, spanning diverse industries including health tech, climate tech, cybersecurity, fintech, and deep tech spinouts focusing on world-class science. In addition, she built the highly successful and economically sustainable Entrepreneurship School , Oxford’s first digital online learning platform. Bakshi comes to MIT after having worked in the private sector as the chief operating officer (COO) in a rapidly growing artificial intelligence startup for almost two years, Quench.ai , with offices in London and New York City. She was the first C-suite employee at Quench.ai, serving as COO and now senior advisor, helping companies unlock value from their knowledge through AI. Right place, right time, right person moving at the speed of MIT AI Since its inception, then turbocharged in the 1940s with the creation and operation of the RadLab , and continuing to this day, entrepreneurship is at the core of MIT’s identity and mission. \"MIT has been a leader in entrepreneurship for decades. It’s now the third leg of the school, alongside teaching and research,” says Mark Gorenberg ’76, chair of the MIT Corporation. “I’m excited to have such a transformative leader as Ana join the Trust Center team, and I look forward to the impact she will have on the students and the wider academic community at MIT as we enter an exciting new phase in company building, driven by the accelerated use of AI and emerging technologies.\" “In a time where we are rethinking management education, entrepreneurship as an interdisciplinary field to create impact is even more important to our future. To have such an experienced and accomplished leader in academia and the startup world, especially in AI, reinforces our commitment to be a global leader in this field,” says Richard M. Locke, John C Head III Dean at the MIT Sloan School of Management. “MIT is a unique hub of research, innovation, and entrepreneurship, and that special mix creates massive positive impact that ripples around the world,” says Frederic Kerrest, MIT Sloan MBA ’09, co-founder of Okta, and member of the MIT Corporation. “In a rapidly changing, AI-driven world, Ana has the skills and experience to further accelerate MIT’s global leadership in entrepreneurship education to ensure that our students launch and scale the next generation of groundbreaking, innovation-driven startups.” Prior to her time at Oxford and King’s College, Bakshi served as an elected councilor representing 6,000-plus constituents, held roles in international nongovernmental organizations, and led product execution strategy at MAHI, an award-winning family-led craft sauce startup, available in thousands of major retailers across the U.K. Bakshi sits on the advisory council for conservation charity Save the Elephants , leveraging AI-driven and scientific approaches to reduce human-wildlife conflict and protect elephant populations. Her work and impact have been featured across FT , Forbes , BBC, The Times , and The Hill . Bakshi was twice honored as a Top 50 Woman in Tech (U.K.), most recently in 2025. “As AI changes how we learn, how we build, and how we scale, my focus will be on helping MIT expand its support for phenomenal talent — students and faculty — with the skills, ecosystem, and backing to turn knowledge into impact,” Bakshi says. 35 years of impact to date The Trust Center was founded in 1990 by the late Professor Edward Roberts and serves all MIT students across all schools and all disciplines. It supports 60-plus courses and extensive extracurricular programming, including the delta v academic accelerator. Much of the work of the center is generated through the Disciplined Entrepreneurship methodology, which offers a proven approach to create new ventures. Over a thousand schools and other organizations across the world use Disciplined Entrepreneurship books and resources to teach entrepreneurship. Now, with AI-powered tools like Orbit and JetPack , the Trust Center is changing the way that entrepreneurship is taught and practiced. Its mission is to produce the next generation of innovation-driven entrepreneurs while advancing the field more broadly to make it both rigorous and practical. This approach of leveraging proven evidence-based methodology, emerging technology, the ingenuity of MIT students, and responding to industry shifts is similar to how MIT established the field of chemical engineering in the 1890s . The desired result in both cases was to create a comprehensive, integrated, scalable, rigorous, and practical curriculum to create a new workforce to address the nation’s and world’s greatest challenges. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Lincoln Lab unveils the most powerful AI supercomputer at any US university",
      "description": "Optimized for generative AI, TX-GAIN is driving innovation in biodefense, materials discovery, cybersecurity, and other areas of research and development.",
      "url": "https://news.mit.edu/2025/lincoln-lab-unveils-most-powerful-ai-supercomputer-at-any-us-university-1002",
      "source": "MIT News AI",
      "published_date": "2025-10-02T19:30:00",
      "author": "Kylie Foy | MIT Lincoln Laboratory",
      "content": "Previous image Next image The new TX-Generative AI Next (TX-GAIN) computing system at the Lincoln Laboratory Supercomputing Center (LLSC) is the most powerful AI supercomputer at any U.S. university. With its recent ranking from TOP500 , which biannually publishes a list of the top supercomputers in various categories, TX-GAIN joins the ranks of other powerful systems at the LLSC, all supporting research and development at Lincoln Laboratory and across the MIT campus. \"TX-GAIN will enable our researchers to achieve scientific and engineering breakthroughs. The system will play a large role in supporting generative AI, physical simulation, and data analysis across all research areas,\" says Lincoln Laboratory Fellow Jeremy Kepner , who heads the LLSC. The LLSC is a key resource for accelerating innovation at Lincoln Laboratory. Thousands of researchers tap into the LLSC to analyze data, train models, and run simulations for federally funded research projects. The supercomputers have been used, for example, to simulate billions of aircraft encounters to develop collision-avoidance systems for the Federal Aviation Administration, and to train models in the complex tasks of autonomous navigation for the Department of Defense. Over the years, LLSC capabilities have been essential to numerous award-winning technologies , including those that have improved airline safety, prevented the spread of new diseases, and aided in hurricane responses. As its name suggests, TX-GAIN is especially equipped for developing and applying generative AI. Whereas traditional AI focuses on categorization tasks, like identifying whether a photo depicts a dog or cat, generative AI produces entirely new outputs. Kepner describes it as a mathematical combination of interpolation (filling in the gaps between known data points) and extrapolation (extending data beyond known points). Today, generative AI is widely known for its use of large language models to create human-like responses to user prompts. At Lincoln Laboratory, teams are applying generative AI to various domains beyond large language models. They are using the technology, for instance, to evaluate radar signatures, supplement weather data where coverage is missing, root out anomalies in network traffic, and explore chemical interactions to design new medicines and materials. To enable such intense computations, TX-GAIN is powered by more than 600 NVIDIA graphics processing unit accelerators specially designed for AI operations, in addition to traditional high-performance computing hardware. With a peak performance of two AI exaflops (two quintillion floating-point operations per second), TX-GAIN is the top AI system at a university, and in the Northeast. Since TX-GAIN came online this summer, researchers have taken notice. \"TX-GAIN is allowing us to model not only significantly more protein interactions than ever before, but also much larger proteins with more atoms. This new computational capability is a game-changer for protein characterization efforts in biological defense,\" says Rafael Jaimes, a researcher in Lincoln Laboratory's Counter–Weapons of Mass Destruction Systems Group . The LLSC's focus on interactive supercomputing makes it especially useful to researchers. For years, the LLSC has pioneered software that lets users access its powerful systems without needing to be experts in configuring algorithms for parallel processing. \"The LLSC has always tried to make supercomputing feel like working on your laptop,\" Kepner says. \"The amount of data and the sophistication of analysis methods needed to be competitive today are well beyond what can be done on a laptop. But with our user-friendly approach, people can run their model and get answers quickly from their workspace.\" Beyond supporting programs solely at Lincoln Laboratory, TX-GAIN is enhancing research collaborations with MIT's campus. Such collaborations include the Haystack Observatory , Center for Quantum Engineering , Beaver Works , and Department of Air Force–MIT AI Accelerator. The latter initiative is rapidly prototyping, scaling, and applying AI technologies for the U.S. Air Force and Space Force, optimizing flight scheduling for global operations as one fielded example. The LLSC systems are housed in an energy-efficient data center and facility in Holyoke, Massachusetts. Research staff in the LLSC are also tackling the immense energy needs of AI and leading research into various power-reduction methods . One software tool they developed can reduce the energy of training an AI model by as much as 80 percent . \"The LLSC provides the capabilities needed to do leading-edge research, while in a cost-effective and energy-efficient manner,\" Kepner says. All of the supercomputers at the LLSC use the \"TX\" nomenclature in homage to Lincoln Laboratory's Transistorized Experimental Computer Zero (TX-0) of 1956. TX-0 was one of the world's first transistor-based machines, and its 1958 successor, TX-2 , is storied for its role in pioneering human-computer interaction and AI. With TX-GAIN, the LLSC continues this legacy. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Responding to the climate impact of generative AI",
      "description": "Explosive growth of AI data centers is expected to increase greenhouse gas emissions. Researchers are now seeking solutions to reduce these environmental harms.",
      "url": "https://news.mit.edu/2025/responding-to-generative-ai-climate-impact-0930",
      "source": "MIT News AI",
      "published_date": "2025-09-30T04:00:00",
      "author": "Adam Zewe | MIT News",
      "content": "Previous image Next image In part 2 of our two-part series on generative artificial intelligence’s environmental impacts , MIT News explores some of the ways experts are working to reduce the technology’s carbon footprint. The energy demands of generative AI are expected to continue increasing dramatically over the next decade. For instance, an April 2025 report from the International Energy Agency predicts that the global electricity demand from data centers , which house the computing infrastructure to train and deploy AI models, will more than double by 2030, to around 945 terawatt-hours. While not all operations performed in a data center are AI-related, this total amount is slightly more than the energy consumption of Japan. Moreover, an August 2025 analysis from Goldman Sachs Research forecasts that about 60 percent of the increasing electricity demands from data centers will be met by burning fossil fuels, increasing global carbon emissions by about 220 million tons . In comparison, driving a gas-powered car for 5,000 miles produces about 1 ton of carbon dioxide. These statistics are staggering, but at the same time, scientists and engineers at MIT and around the world are studying innovations and interventions to mitigate AI’s ballooning carbon footprint, from boosting the efficiency of algorithms to rethinking the design of data centers. Considering carbon emissions Talk of reducing generative AI’s carbon footprint is typically centered on “operational carbon” — the emissions used by the powerful processors, known as GPUs, inside a data center. It often ignores “embodied carbon,” which are emissions created by building the data center in the first place, says Vijay Gadepally, senior scientist at MIT Lincoln Laboratory, who leads research projects in the Lincoln Laboratory Supercomputing Center. Constructing and retrofitting a data center, built from tons of steel and concrete and filled with air conditioning units, computing hardware, and miles of cable, consumes a huge amount of carbon. In fact, the environmental impact of building data centers is one reason companies like Meta and Google are exploring more sustainable building materials. (Cost is another factor.) Plus, data centers are enormous buildings — the world’s largest, the China Telecomm-Inner Mongolia Information Park, engulfs roughly 10 million square feet — with about 10 to 50 times the energy density of a normal office building, Gadepally adds. “The operational side is only part of the story. Some things we are working on to reduce operational emissions may lend themselves to reducing embodied carbon, too, but we need to do more on that front in the future,” he says. Reducing operational carbon emissions When it comes to reducing operational carbon emissions of AI data centers, there are many parallels with home energy-saving measures. For one, we can simply turn down the lights. “Even if you have the worst lightbulbs in your house from an efficiency standpoint, turning them off or dimming them will always use less energy than leaving them running at full blast,” Gadepally says. In the same fashion, research from the Supercomputing Center has shown that “turning down” the GPUs in a data center so they consume about three-tenths the energy has minimal impacts on the performance of AI models, while also making the hardware easier to cool. Another strategy is to use less energy-intensive computing hardware. Demanding generative AI workloads, such as training new reasoning models like GPT-5, usually need many GPUs working simultaneously. The Goldman Sachs analysis estimates that a state-of-the-art system could soon have as many as 576 connected GPUs operating at once. But engineers can sometimes achieve similar results by reducing the precision of computing hardware, perhaps by switching to less powerful processors that have been tuned to handle a specific AI workload. There are also measures that boost the efficiency of training power-hungry deep-learning models before they are deployed. Gadepally’s group found that about half the electricity used for training an AI model is spent to get the last 2 or 3 percentage points in accuracy. Stopping the training process early can save a lot of that energy. “There might be cases where 70 percent accuracy is good enough for one particular application, like a recommender system for e-commerce,” he says. Researchers can also take advantage of efficiency-boosting measures. For instance, a postdoc in the Supercomputing Center realized the group might run a thousand simulations during the training process to pick the two or three best AI models for their project. By building a tool that allowed them to avoid about 80 percent of those wasted computing cycles, they dramatically reduced the energy demands of training with no reduction in model accuracy, Gadepally says. Leveraging efficiency improvements Constant innovation in computing hardware, such as denser arrays of transistors on semiconductor chips, is still enabling dramatic improvements in the energy efficiency of AI models. Even though energy efficiency improvements have been slowing for most chips since about 2005, the amount of computation that GPUs can do per joule of energy has been improving by 50 to 60 percent each year, says Neil Thompson, director of the FutureTech Research Project at MIT’s Computer Science and Artificial Intelligence Laboratory and a principal investigator at MIT’s Initiative on the Digital Economy. “The still-ongoing ‘Moore’s Law’ trend of getting more and more transistors on chip still matters for a lot of these AI systems, since running operations in parallel is still very valuable for improving efficiency,” says Thomspon. Even more significant, his group’s research indicates that efficiency gains from new model architectures that can solve complex problems faster, consuming less energy to achieve the same or better results, is doubling every eight or nine months. Thompson coined the term “ negaflop ” to describe this effect. The same way a “negawatt” represents electricity saved due to energy-saving measures, a “negaflop” is a computing operation that doesn’t need to be performed due to algorithmic improvements. These could be things like “ pruning ” away unnecessary components of a neural network or employing compression techniques that enable users to do more with less computation. “If you need to use a really powerful model today to complete your task, in just a few years, you might be able to use a significantly smaller model to do the same thing, which would carry much less environmental burden. Making these models more efficient is the single-most important thing you can do to reduce the environmental costs of AI,” Thompson says. Maximizing energy savings While reducing the overall energy use of AI algorithms and computing hardware will cut greenhouse gas emissions, not all energy is the same, Gadepally adds. “The amount of carbon emissions in 1 kilowatt hour varies quite significantly, even just during the day, as well as over the month and year,” he says. Engineers can take advantage of these variations by leveraging the flexibility of AI workloads and data center operations to maximize emissions reductions. For instance, some generative AI workloads don’t need to be performed in their entirety at the same time. Splitting computing operations so some are performed later, when more of the electricity fed into the grid is from renewable sources like solar and wind, can go a long way toward reducing a data center’s carbon footprint, says Deepjyoti Deka, a research scientist in the MIT Energy Initiative. Deka and his team are also studying “smarter” data centers where the AI workloads of multiple companies using the same computing equipment are flexibly adjusted to improve energy efficiency. “By looking at the system as a whole, our hope is to minimize energy use as well as dependence on fossil fuels, while still maintaining reliability standards for AI companies and users,” Deka says. He and others at MITEI are building a flexibility model of a data center that considers the differing energy demands of training a deep-learning model versus deploying that model. Their hope is to uncover the best strategies for scheduling and streamlining computing operations to improve energy efficiency. The researchers are also exploring the use of long-duration energy storage units at data centers, which store excess energy for times when it is needed. With these systems in place, a data center could use stored energy that was generated by renewable sources during a high-demand period, or avoid the use of diesel backup generators if there are fluctuations in the grid. “Long-duration energy storage could be a game-changer here because we can design operations that really change the emission mix of the system to rely more on renewable energy,” Deka says. In addition, researchers at MIT and Princeton University are developing a software tool for investment planning in the power sector, called GenX , which could be used to help companies determine the ideal place to locate a data center to minimize environmental impacts and costs. Location can have a big impact on reducing a data center’s carbon footprint. For instance, Meta operates a data center in Lulea , a city on the coast of northern Sweden where cooler temperatures reduce the amount of electricity needed to cool computing hardware. Thinking farther outside the box (way farther), some governments are even exploring the construction of data centers on the moon where they could potentially be operated with nearly all renewable energy. Currently, the expansion of renewable energy generation here on Earth isn’t keeping pace with the rapid growth of AI, which is one major roadblock to reducing its carbon footprint, says Jennifer Turliuk MBA ’25, a short-term lecturer, former Sloan Fellow, and former practice leader of climate and energy AI at the Martin Trust Center for MIT Entrepreneurship. The local, state, and federal review processes required for a new renewable energy projects can take years. Researchers at MIT and elsewhere are exploring the use of AI to speed up the process of connecting new renewable energy systems to the power grid. For instance, a generative AI model could streamline interconnection studies that determine how a new project will impact the power grid, a step that often takes years to complete. And when it comes to accelerating the development and implementation of clean energy technologies , AI could play a major role. “Machine learning is great for tackling complex situations, and the electrical grid is said to be one of the largest and most complex machines in the world,” Turliuk adds. For instance, AI could help optimize the prediction of solar and wind energy generation or identify ideal locations for new facilities. It could also be used to perform predictive maintenance and fault detection for solar panels or other green energy infrastructure, or to monitor the capacity of transmission wires to maximize efficiency. By helping researchers gather and analyze huge amounts of data, AI could also inform targeted policy interventions aimed at getting the biggest “bang for the buck” from areas such as renewable energy, Turliuk says. To help policymakers, scientists, and enterprises consider the multifaceted costs and benefits of AI systems, she and her collaborators developed the Net Climate Impact Score. The score is a framework that can be used to help determine the net climate impact of AI projects, considering emissions and other environmental costs along with potential environmental benefits in the future. At the end of the day, the most effective solutions will likely result from collaborations among companies, regulators, and researchers, with academia leading the way, Turliuk adds. “Every day counts. We are on a path where the effects of climate change won’t be fully known until it is too late to do anything about it. This is a once-in-a-lifetime opportunity to innovate and make AI systems less carbon-intense,” she says. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "AI system learns from many types of scientific information and runs experiments to discover new materials",
      "description": "The new “CRESt” platform could help find solutions to real-world energy problems that have plagued the materials science and engineering community for decades.",
      "url": "https://news.mit.edu/2025/ai-system-learns-many-types-scientific-information-and-runs-experiments-discovering-new-materials-0925",
      "source": "MIT News AI",
      "published_date": "2025-09-25T15:00:00",
      "author": "Zach Winn | MIT News",
      "content": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license . You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided below, credit the images to \"MIT.\" Previous image Next image Machine-learning models can speed up the discovery of new materials by making predictions and suggesting experiments. But most models today only consider a few specific types of data or variables. Compare that with human scientists, who work in a collaborative environment and consider experimental results, the broader scientific literature, imaging and structural analysis, personal experience or intuition, and input from colleagues and peer reviewers. Now, MIT researchers have developed a method for optimizing materials recipes and planning experiments that incorporates information from diverse sources like insights from the literature, chemical compositions, microstructural images, and more. The approach is part of a new platform, named Copilot for Real-world Experimental Scientists (CRESt), that also uses robotic equipment for high-throughput materials testing, the results of which are fed back into large multimodal models to further optimize materials recipes. Human researchers can converse with the system in natural language, with no coding required, and the system makes its own observations and hypotheses along the way. Cameras and visual language models also allow the system to monitor experiments, detect issues, and suggest corrections. “In the field of AI for science, the key is designing new experiments,” says Ju Li, School of Engineering Carl Richard Soderberg Professor of Power Engineering. “We use multimodal feedback — for example information from previous literature on how palladium behaved in fuel cells at this temperature, and human feedback — to complement experimental data and design new experiments. We also use robots to synthesize and characterize the material’s structure and to test performance.” The system is described in a paper published in Nature . The researchers used CRESt to explore more than 900 chemistries and conduct 3,500 electrochemical tests, leading to the discovery of a catalyst material that delivered record power density in a fuel cell that runs on formate salt to produce electricity. Joining Li on the paper as first authors are PhD student Zhen Zhang, Zhichu Ren PhD ’24, PhD student Chia-Wei Hsu, and postdoc Weibin Chen. Their coauthors are MIT Assistant Professor Iwnetim Abate; Associate Professor Pulkit Agrawal; JR East Professor of Engineering Yang Shao-Horn; MIT.nano researcher Aubrey Penn; Zhang-Wei Hong PhD ’25, Hongbin Xu PhD ’25; Daniel Zheng PhD ’25; MIT graduate students Shuhan Miao and Hugh Smith; MIT postdocs Yimeng Huang, Weiyin Chen, Yungsheng Tian, Yifan Gao, and Yaoshen Niu; former MIT postdoc Sipei Li; and collaborators including Chi-Feng Lee, Yu-Cheng Shao, Hsiao-Tsu Wang, and Ying-Rui Lu. A smarter system Materials science experiments can be time-consuming and expensive. They require researchers to carefully design workflows, make new material, and run a series of tests and analysis to understand what happened. Those results are then used to decide how to improve the material. To improve the process, some researchers have turned to a machine-learning strategy known as active learning to make efficient use of previous experimental data points and explore or exploit those data. When paired with a statistical technique known as Bayesian optimization (BO), active learning has helped researchers identify new materials for things like batteries and advanced semiconductors. “Bayesian optimization is like Netflix recommending the next movie to watch based on your viewing history, except instead it recommends the next experiment to do,” Li explains. “But basic Bayesian optimization is too simplistic. It uses a boxed-in design space, so if I say I’m going to use platinum, palladium, and iron, it only changes the ratio of those elements in this small space. But real materials have a lot more dependencies, and BO often gets lost.” Most active learning approaches also rely on single data streams that don’t capture everything that goes on in an experiment. To equip computational systems with more human-like knowledge, while still taking advantage of the speed and control of automated systems, Li and his collaborators built CRESt. CRESt’s robotic equipment includes a liquid-handling robot, a carbothermal shock system to rapidly synthesize materials, an automated electrochemical workstation for testing, characterization equipment including automated electron microscopy and optical microscopy, and auxiliary devices such as pumps and gas valves, which can also be remotely controlled. Many processing parameters can also be tuned. With the user interface, researchers can chat with CRESt and tell it to use active learning to find promising materials recipes for different projects. CRESt can include up to 20 precursor molecules and substrates into its recipe. To guide material designs, CRESt’s models search through scientific papers for descriptions of elements or precursor molecules that might be useful. When human researchers tell CRESt to pursue new recipes, it kicks off a robotic symphony of sample preparation, characterization, and testing. The researcher can also ask CRESt to perform image analysis from scanning electron microscopy imaging, X-ray diffraction, and other sources. Information from those processes is used to train the active learning models, which use both literature knowledge and current experimental results to suggest further experiments and accelerate materials discovery. “For each recipe we use previous literature text or databases, and it creates these huge representations of every recipe based on the previous knowledge base before even doing the experiment,” says Li. “We perform principal component analysis in this knowledge embedding space to get a reduced search space that captures most of the performance variability. Then we use Bayesian optimization in this reduced space to design the new experiment. After the new experiment, we feed newly acquired multimodal experimental data and human feedback into a large language model to augment the knowledgebase and redefine the reduced search space, which gives us a big boost in active learning efficiency.” Materials science experiments can also face reproducibility challenges. To address the problem, CRESt monitors its experiments with cameras, looking for potential problems and suggesting solutions via text and voice to human researchers. The researchers used CRESt to develop an electrode material for an advanced type of high-density fuel cell known as a direct formate fuel cell. After exploring more than 900 chemistries over three months, CRESt discovered a catalyst material made from eight elements that achieved a 9.3-fold improvement in power density per dollar over pure palladium, an expensive precious metal. In further tests, CRESTs material was used to deliver a record power density to a working direct formate fuel cell even though the cell contained just one-fourth of the precious metals of previous devices. The results show the potential for CRESt to find solutions to real-world energy problems that have plagued the materials science and engineering community for decades. “A significant challenge for fuel-cell catalysts is the use of precious metal,” says Zhang. “For fuel cells, researchers have used various precious metals like palladium and platinum. We used a multielement catalyst that also incorporates many other cheap elements to create the optimal coordination environment for catalytic activity and resistance to poisoning species such as carbon monoxide and adsorbed hydrogen atom. People have been searching low-cost options for many years. This system greatly accelerated our search for these catalysts.” A helpful assistant Early on, poor reproducibility emerged as a major problem that limited the researchers’ ability to perform their new active learning technique on experimental datasets. Material properties can be influenced by the way the precursors are mixed and processed, and any number of problems can subtly alter experimental conditions, requiring careful inspection to correct. To partially automate the process, the researchers coupled computer vision and vision language models with domain knowledge from the scientific literature, which allowed the system to hypothesize sources of irreproducibility and propose solutions. For example, the models can notice when there’s a millimeter-sized deviation in a sample’s shape or when a pipette moves something out of place. The researchers incorporated some of the model’s suggestions, leading to improved consistency, suggesting the models already make good experimental assistants. The researchers noted that humans still performed most of the debugging in their experiments. “CREST is an assistant, not a replacement, for human researchers,” Li says. “Human researchers are still indispensable. In fact, we use natural language so the system can explain what it is doing and present observations and hypotheses. But this is a step toward more flexible, self-driving labs.” Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "New AI system could accelerate clinical research",
      "description": "By enabling rapid annotation of areas of interest in medical images, the tool can help scientists study new treatments or map disease progression.",
      "url": "https://news.mit.edu/2025/new-ai-system-could-accelerate-clinical-research-0925",
      "source": "MIT News AI",
      "published_date": "2025-09-25T04:00:00",
      "author": "Adam Zewe | MIT News",
      "content": "Previous image Next image Annotating regions of interest in medical images, a process known as segmentation, is often one of the first steps clinical researchers take when running a new study involving biomedical images. For instance, to determine how the size of the brain’s hippocampus changes as patients age, the scientist first outlines each hippocampus in a series of brain scans. For many structures and image types, this is often a manual process that can be extremely time-consuming, especially if the regions being studied are challenging to delineate. To streamline the process, MIT researchers developed an artificial intelligence-based system that enables a researcher to rapidly segment new biomedical imaging datasets by clicking, scribbling, and drawing boxes on the images. This new AI model uses these interactions to predict the segmentation. As the user marks additional images, the number of interactions they need to perform decreases, eventually dropping to zero. The model can then segment each new image accurately without user input. It can do this because the model’s architecture has been specially designed to use information from images it has already segmented to make new predictions. Unlike other medical image segmentation models, this system allows the user to segment an entire dataset without repeating their work for each image. In addition, the interactive tool does not require a presegmented image dataset for training, so users don’t need machine-learning expertise or extensive computational resources. They can use the system for a new segmentation task without retraining the model. In the long run, this tool could accelerate studies of new treatment methods and reduce the cost of clinical trials and medical research. It could also be used by physicians to improve the efficiency of clinical applications, such as radiation treatment planning. “Many scientists might only have time to segment a few images per day for their research because manual image segmentation is so time-consuming. Our hope is that this system will enable new science by allowing clinical researchers to conduct studies they were prohibited from doing before because of the lack of an efficient tool,” says Hallee Wong, an electrical engineering and computer science graduate student and lead author of a paper on this new tool . She is joined on the paper by Jose Javier Gonzalez Ortiz PhD ’24; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering; and senior author Adrian Dalca, an assistant professor at Harvard Medical School and MGH, and a research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference on Computer Vision. There are primarily two methods researchers use to segment new sets of medical images. With interactive segmentation, they input an image into an AI system and use an interface to mark areas of interest. The model predicts the segmentation based on those interactions. A tool previously developed by the MIT researchers, ScribblePrompt , allows users to do this, but they must repeat the process for each new image. Another approach is to develop a task-specific AI model to automatically segment the images. This approach requires the user to manually segment hundreds of images to create a dataset, and then train a machine-learning model. That model predicts the segmentation for a new image. But the user must start the complex, machine-learning-based process from scratch for each new task, and there is no way to correct the model if it makes a mistake. This new system, MultiverSeg , combines the best of each approach. It predicts a segmentation for a new image based on user interactions, like scribbles, but also keeps each segmented image in a context set that it refers to later. When the user uploads a new image and marks areas of interest, the model draws on the examples in its context set to make a more accurate prediction, with less user input. The researchers designed the model’s architecture to use a context set of any size, so the user doesn’t need to have a certain number of images. This gives MultiverSeg the flexibility to be used in a range of applications. “At some point, for many tasks, you shouldn’t need to provide any interactions. If you have enough examples in the context set, the model can accurately predict the segmentation on its own,” Wong says. The researchers carefully engineered and trained the model on a diverse collection of biomedical imaging data to ensure it had the ability to incrementally improve its predictions based on user input. The user doesn’t need to retrain or customize the model for their data. To use MultiverSeg for a new task, one can upload a new medical image and start marking it. When the researchers compared MultiverSeg to state-of-the-art tools for in-context and interactive image segmentation, it outperformed each baseline. Fewer clicks, better results Unlike these other tools, MultiverSeg requires less user input with each image. By the ninth new image, it needed only two clicks from the user to generate a segmentation more accurate than a model designed specifically for the task. For some image types, like X-rays, the user might only need to segment one or two images manually before the model becomes accurate enough to make predictions on its own. The tool’s interactivity also enables the user to make corrections to the model’s prediction, iterating until it reaches the desired level of accuracy. Compared to the researchers’ previous system, MultiverSeg reached 90 percent accuracy with roughly 2/3 the number of scribbles and 3/4 the number of clicks. “With MultiverSeg, users can always provide more interactions to refine the AI predictions. This still dramatically accelerates the process because it is usually faster to correct something that exists than to start from scratch,” Wong says. Moving forward, the researchers want to test this tool in real-world situations with clinical collaborators and improve it based on user feedback. They also want to enable MultiverSeg to segment 3D biomedical images. This work is supported, in part, by Quanta Computer, Inc. and the National Institutes of Health, with hardware support from the Massachusetts Life Sciences Center. Previous item Next item",
      "category": "AI/Tech"
    },
    {
      "title": "Disrupting malicious uses of AI: October 2025",
      "description": "Discover how OpenAI is detecting and disrupting malicious uses of AI in our October 2025 report. Learn how we’re countering misuse, enforcing policies, and protecting users from real-world harms.",
      "url": "https://openai.com/global-affairs/disrupting-malicious-uses-of-ai-october-2025",
      "source": "OpenAI Blog",
      "published_date": "2025-10-07T03:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Codex is now generally available",
      "description": "OpenAI Codex is now generally available with powerful new features for developers: a Slack integration, Codex SDK, and admin tools like usage dashboards and workspace management—making Codex easier to use and manage at scale.",
      "url": "https://openai.com/index/codex-now-generally-available",
      "source": "OpenAI Blog",
      "published_date": "2025-10-06T10:50:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Introducing apps in ChatGPT and the new Apps SDK",
      "description": "We’re introducing a new generation of apps you can chat with, right inside ChatGPT. Developers can start building them today with the new Apps SDK, available in preview.",
      "url": "https://openai.com/index/introducing-apps-in-chatgpt",
      "source": "OpenAI Blog",
      "published_date": "2025-10-06T10:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "AMD and OpenAI announce strategic partnership to deploy 6 gigawatts of AMD GPUs",
      "description": "AMD and OpenAI have announced a multi-year partnership to deploy 6 gigawatts of AMD Instinct GPUs, beginning with 1 gigawatt in 2026, to power OpenAI’s next-generation AI infrastructure and accelerate global AI innovation.",
      "url": "https://openai.com/index/openai-amd-strategic-partnership",
      "source": "OpenAI Blog",
      "published_date": "2025-10-06T06:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Introducing AgentKit, new Evals, and RFT for agents",
      "description": "Today, we’re releasing new tools to help developers go from prototype to production faster: AgentKit, expanded evals capabilities, and reinforcement fine-tuning for agents.",
      "url": "https://openai.com/index/introducing-agentkit",
      "source": "OpenAI Blog",
      "published_date": "2025-10-06T00:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "Accelerating AI adoption in Europe",
      "description": "OpenAI and Allied for Startups release the Hacktivate AI report with 20 actionable policy ideas to accelerate AI adoption in Europe, boost competitiveness, and empower innovators.",
      "url": "https://openai.com/global-affairs/accelerating-ai-uptake-in-europe",
      "source": "OpenAI Blog",
      "published_date": "2025-10-06T00:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "With GPT-5, Wrtn builds lifestyle AI for millions in Korea",
      "description": "Wrtn scaled AI apps to 6.5M users in Korea with GPT-5, creating ‘Lifestyle AI’ that blends productivity, creativity, and learning—now expanding across East Asia.",
      "url": "https://openai.com/index/wrtn",
      "source": "OpenAI Blog",
      "published_date": "2025-10-02T10:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    },
    {
      "title": "OpenAI announces strategic collaboration with Japan’s Digital Agency",
      "description": "OpenAI and Japan’s Digital Agency partner to advance generative AI in public services, support international AI governance, and promote safe, trustworthy AI adoption worldwide.",
      "url": "https://openai.com/global-affairs/strategic-collaboration-with-japan-digital-agency",
      "source": "OpenAI Blog",
      "published_date": "2025-10-02T00:00:00",
      "author": null,
      "content": "",
      "category": "AI/Tech"
    }
  ]
}