#!/usr/bin/env python3
"""
News Scraper Gen√©rico
- Por defecto: corre todas las categor√≠as
- Con flag: corre solo categor√≠as espec√≠ficas
- URLs definidos en categories_config.json
"""

import argparse
import sys
import json # <--- IMPORTACI√ìN NECESARIA PARA JSON
from enhanced_category_scraper import EnhancedCategoryScraper

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='News Scraper - Extrae noticias por categor√≠a',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python3 news_scraper.py                  # Correr todas las categor√≠as
  python3 news_scraper.py --category Marketing  # Solo Marketing
  python3 news_scraper.py --category "AI/Tech"  # Solo AI/Tech
  python3 news_scraper.py --category Marketing --category "AI/Tech"  # M√∫ltiples categor√≠as
        """
    )
    
    parser.add_argument(
        '--category', '-c',
        action='append',
        help='Categor√≠a espec√≠fica a procesar (puede usarse m√∫ltiples veces)'
    )
    
    parser.add_argument(
        '--list-categories', '-l',
        action='store_true',
        help='Listar categor√≠as disponibles y salir'
    )
    
    return parser.parse_args()

# --- NUEVA FUNCI√ìN PARA IMPRIMIR JSON PURO A STDOUT ---
def print_final_json(results):
    """Imprime el resultado final en formato JSON a stdout (para n8n/curl)."""
    try:
        json_output = json.dumps(results, ensure_ascii=False, indent=None)
        sys.stdout.write(json_output)
        sys.stdout.flush() # Asegura que el JSON se escriba inmediatamente
    except Exception as e:
        sys.stderr.write(f"‚ùå Error al escribir el JSON a stdout: {e}\n")
        sys.exit(1)
# ----------------------------------------------------

def list_available_categories():
    """List available categories from config"""
    try:
        # Importamos json aqu√≠ solo si se usa, pero ya est√° arriba
        with open('categories_config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ***** CAMBIOS: Usar sys.stderr.write para los logs *****
        sys.stderr.write("üìÇ Categor√≠as disponibles:\n")
        sys.stderr.write("=" * 30 + "\n")
        for category_name, category_data in config['categories'].items():
            sys.stderr.write(f"‚Ä¢ {category_name}\n")
            sys.stderr.write(f"  üìù {category_data['description']}\n")
            sys.stderr.write(f"  üîó {len(category_data['urls'])} URLs\n")
            sys.stderr.write("\n")
    except FileNotFoundError:
        sys.stderr.write("‚ùå No se encontr√≥ categories_config.json\n")
    except Exception as e:
        sys.stderr.write(f"‚ùå Error leyendo configuraci√≥n: {e}\n")

def main_cli():
    """Main CLI function"""
    args = parse_arguments()
    
    # List categories and exit
    if args.list_categories:
        list_available_categories()
        return
    
    # Determine categories to scrape
    if args.category:
        categories_to_scrape = args.category
        # [CORREGIDO]
        sys.stderr.write(f"üéØ Procesando categor√≠as: {', '.join(categories_to_scrape)}\n")
    else:
        categories_to_scrape = None
        # [CORREGIDO] La l√≠nea que causaba el error con el emoji 'üîÑ'
        sys.stderr.write("üîÑ Procesando todas las categor√≠as\n")
    
    # Run scraper
    try:
        scraper = EnhancedCategoryScraper()
        
        if categories_to_scrape:
            # Scrape specific categories
            results = {}
            for category in categories_to_scrape:
                articles = scraper.scrape_category(category, max_articles_per_source=8)
                results[category] = articles
        else:
            # Scrape all categories
            results = scraper.scrape_all_categories(max_articles_per_source=8)
        
        scraper.results = results
        
        # Print summary (ASUME que print_summary() ha sido corregido en el otro archivo)
        scraper.print_summary()
        
        # Save results
        # [CORREGIDO]
        sys.stderr.write(f"\nüíæ Saving results to organized folders...\n")
        scraper.save_results()
        
        if results:
            # AHORA IMPRIMIMOS EL JSON A STDOUT
            print_final_json(results)
            
            # Mensajes de √©xito y m√©tricas (logs a stderr)
            # [CORREGIDO]
            sys.stderr.write(f"\n‚úÖ Scraping completado exitosamente!\n")
            total_articles = sum(len(articles) for articles in results.values())
            sys.stderr.write(f"üìä Total de art√≠culos: {total_articles}\n")
        else:
            # [CORREGIDO]
            sys.stderr.write("‚ùå No se encontraron art√≠culos\n")
            sys.exit(1)
            
    except KeyboardInterrupt:
        # [CORREGIDO]
        sys.stderr.write("\n‚èπÔ∏è  Scraping cancelado por el usuario\n")
        sys.exit(1)
    except Exception as e:
        # [CORREGIDO]
        sys.stderr.write(f"‚ùå Error durante el scraping: {e}\n")
        sys.exit(1)

if __name__ == "__main__":
    main_cli()
