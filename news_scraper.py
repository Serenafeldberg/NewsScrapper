#!/usr/bin/env python3
"""
News Scraper Gen√©rico
- Por defecto: corre todas las categor√≠as
- Con flag: corre solo categor√≠as espec√≠ficas
- URLs definidos en categories_config.json
"""

import argparse
import sys
from enhanced_category_scraper import EnhancedCategoryScraper

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='News Scraper - Extrae noticias por categor√≠a',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python3 news_scraper.py                    # Correr todas las categor√≠as
  python3 news_scraper.py --category Marketing  # Solo Marketing
  python3 news_scraper.py --category "AI/Tech"  # Solo AI/Tech
  python3 news_scraper.py --category Marketing --category "AI/Tech"  # M√∫ltiples categor√≠as
        """
    )
    
    parser.add_argument(
        '--category', '-c',
        action='append',
        help='Categor√≠a espec√≠fica a procesar (puede usarse m√∫ltiples veces)'
    )
    
    parser.add_argument(
        '--list-categories', '-l',
        action='store_true',
        help='Listar categor√≠as disponibles y salir'
    )
    
    return parser.parse_args()

def list_available_categories():
    """List available categories from config"""
    try:
        import json
        with open('categories_config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print("üìÇ Categor√≠as disponibles:")
        print("=" * 30)
        for category_name, category_data in config['categories'].items():
            print(f"‚Ä¢ {category_name}")
            print(f"  üìù {category_data['description']}")
            print(f"  üîó {len(category_data['urls'])} URLs")
            print()
    except FileNotFoundError:
        print("‚ùå No se encontr√≥ categories_config.json")
    except Exception as e:
        print(f"‚ùå Error leyendo configuraci√≥n: {e}")

def main_cli():
    """Main CLI function"""
    args = parse_arguments()
    
    # List categories and exit
    if args.list_categories:
        list_available_categories()
        return
    
    # Determine categories to scrape
    if args.category:
        categories_to_scrape = args.category
        print(f"üéØ Procesando categor√≠as: {', '.join(categories_to_scrape)}")
    else:
        categories_to_scrape = None
        print("üîÑ Procesando todas las categor√≠as")
    
    # Run scraper
    try:
        scraper = EnhancedCategoryScraper()
        
        if categories_to_scrape:
            # Scrape specific categories
            results = {}
            for category in categories_to_scrape:
                articles = scraper.scrape_category(category, max_articles_per_source=8)
                results[category] = articles
        else:
            # Scrape all categories
            results = scraper.scrape_all_categories(max_articles_per_source=8)
        
        scraper.results = results
        
        # Print summary
        scraper.print_summary()
        
        # Save results
        print(f"\nüíæ Saving results to organized folders...")
        scraper.save_results()
        
        if results:
            print(f"\n‚úÖ Scraping completado exitosamente!")
            total_articles = sum(len(articles) for articles in results.values())
            print(f"üìä Total de art√≠culos: {total_articles}")
        else:
            print("‚ùå No se encontraron art√≠culos")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Scraping cancelado por el usuario")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error durante el scraping: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main_cli()
